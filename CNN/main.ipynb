{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework-3: ConvNet for MNIST Classification\n",
    "\n",
    "### In this homework, you need to\n",
    "- #### implement the forward and backward functions for ConvLayer (`layers/conv_layer.py`)\n",
    "- #### implement the forward and backward functions for PoolingLayer (`layers/pooling_layer.py`)\n",
    "- #### implement the forward and backward functions for DropoutLayer (`layers/dropout_layer.py`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from network import Network\n",
    "from solver import train, test\n",
    "from plot import plot_loss_and_acc\n",
    "from mnist_data_loader import read_data_sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load MNIST Dataset\n",
    "We use mnist_data_loader.py to load the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data\\train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data\\train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data\\t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "dataset = read_data_sets(\"MNIST_data\", one_hot=True, validation_size=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: Training 55000, Validation 5000, Test 10000\n"
     ]
    }
   ],
   "source": [
    "# subtract by the mean value\n",
    "mean_val = dataset.train.images.mean()\n",
    "dataset.train._images -= mean_val\n",
    "dataset.test._images -= mean_val\n",
    "dataset.validation._images -= mean_val\n",
    "\n",
    "# reshape to Nx1x28x28\n",
    "dataset.train._images = dataset.train._images.reshape(-1, 1, 28, 28)\n",
    "dataset.test._images = dataset.test._images.reshape(-1, 1, 28, 28)\n",
    "dataset.validation._images = dataset.validation._images.reshape(-1, 1, 28, 28)\n",
    "\n",
    "print(\"Dataset size: Training {}, Validation {}, Test {}\"\\\n",
    "    .format(dataset.train.num_examples, dataset.validation.num_examples,\n",
    "            dataset.test.num_examples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 75\n",
    "max_epoch = 5\n",
    "init_std = 0.01\n",
    "\n",
    "learning_rate = 0.001\n",
    "weight_decay = 0.010\n",
    "\n",
    "disp_freq = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criterion and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from criterion import SoftmaxCrossEntropyLossLayer\n",
    "from optimizer import SGD\n",
    "\n",
    "criterion = SoftmaxCrossEntropyLossLayer()\n",
    "sgd = SGD(learning_rate, weight_decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ConvNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from layers import FCLayer, ReLULayer, ConvLayer, MaxPoolingLayer, ReshapeLayer\n",
    "\n",
    "convNet = Network()\n",
    "convNet.add(ConvLayer(1, 8, 3, 1))\n",
    "convNet.add(ReLULayer())\n",
    "convNet.add(MaxPoolingLayer(2, 0))\n",
    "convNet.add(ConvLayer(8, 16, 3, 1))\n",
    "convNet.add(ReLULayer())\n",
    "convNet.add(MaxPoolingLayer(2, 0))\n",
    "convNet.add(ReshapeLayer((batch_size, 16, 7, 7), (batch_size, 784)))\n",
    "convNet.add(FCLayer(784, 128))\n",
    "convNet.add(ReLULayer())\n",
    "convNet.add(FCLayer(128, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0][5]\t Batch [0][733]\t Training Loss 19.3751\t Accuracy 0.0533\n",
      "Epoch [0][5]\t Batch [10][733]\t Training Loss 16.6967\t Accuracy 0.1321\n",
      "Epoch [0][5]\t Batch [20][733]\t Training Loss 12.5074\t Accuracy 0.1854\n",
      "Epoch [0][5]\t Batch [30][733]\t Training Loss 10.6566\t Accuracy 0.2000\n",
      "Epoch [0][5]\t Batch [40][733]\t Training Loss 9.3118\t Accuracy 0.2094\n",
      "Epoch [0][5]\t Batch [50][733]\t Training Loss 8.1750\t Accuracy 0.2319\n",
      "Epoch [0][5]\t Batch [60][733]\t Training Loss 7.3357\t Accuracy 0.2533\n",
      "Epoch [0][5]\t Batch [70][733]\t Training Loss 6.6522\t Accuracy 0.2719\n",
      "Epoch [0][5]\t Batch [80][733]\t Training Loss 6.0974\t Accuracy 0.2942\n",
      "Epoch [0][5]\t Batch [90][733]\t Training Loss 5.7141\t Accuracy 0.3051\n",
      "Epoch [0][5]\t Batch [100][733]\t Training Loss 5.3801\t Accuracy 0.3151\n",
      "Epoch [0][5]\t Batch [110][733]\t Training Loss 5.1313\t Accuracy 0.3202\n",
      "Epoch [0][5]\t Batch [120][733]\t Training Loss 4.8771\t Accuracy 0.3296\n",
      "Epoch [0][5]\t Batch [130][733]\t Training Loss 4.6858\t Accuracy 0.3332\n",
      "Epoch [0][5]\t Batch [140][733]\t Training Loss 4.4872\t Accuracy 0.3407\n",
      "Epoch [0][5]\t Batch [150][733]\t Training Loss 4.3129\t Accuracy 0.3485\n",
      "Epoch [0][5]\t Batch [160][733]\t Training Loss 4.1513\t Accuracy 0.3566\n",
      "Epoch [0][5]\t Batch [170][733]\t Training Loss 4.0262\t Accuracy 0.3606\n",
      "Epoch [0][5]\t Batch [180][733]\t Training Loss 3.8978\t Accuracy 0.3681\n",
      "Epoch [0][5]\t Batch [190][733]\t Training Loss 3.7792\t Accuracy 0.3751\n",
      "Epoch [0][5]\t Batch [200][733]\t Training Loss 3.6727\t Accuracy 0.3824\n",
      "Epoch [0][5]\t Batch [210][733]\t Training Loss 3.5708\t Accuracy 0.3898\n",
      "Epoch [0][5]\t Batch [220][733]\t Training Loss 3.4818\t Accuracy 0.3952\n",
      "Epoch [0][5]\t Batch [230][733]\t Training Loss 3.3989\t Accuracy 0.3997\n",
      "Epoch [0][5]\t Batch [240][733]\t Training Loss 3.3292\t Accuracy 0.4024\n",
      "Epoch [0][5]\t Batch [250][733]\t Training Loss 3.2530\t Accuracy 0.4081\n",
      "Epoch [0][5]\t Batch [260][733]\t Training Loss 3.1869\t Accuracy 0.4130\n",
      "Epoch [0][5]\t Batch [270][733]\t Training Loss 3.1218\t Accuracy 0.4181\n",
      "Epoch [0][5]\t Batch [280][733]\t Training Loss 3.0553\t Accuracy 0.4239\n",
      "Epoch [0][5]\t Batch [290][733]\t Training Loss 2.9972\t Accuracy 0.4283\n",
      "Epoch [0][5]\t Batch [300][733]\t Training Loss 2.9486\t Accuracy 0.4310\n",
      "Epoch [0][5]\t Batch [310][733]\t Training Loss 2.8954\t Accuracy 0.4350\n",
      "Epoch [0][5]\t Batch [320][733]\t Training Loss 2.8477\t Accuracy 0.4390\n",
      "Epoch [0][5]\t Batch [330][733]\t Training Loss 2.8047\t Accuracy 0.4422\n",
      "Epoch [0][5]\t Batch [340][733]\t Training Loss 2.7683\t Accuracy 0.4448\n",
      "Epoch [0][5]\t Batch [350][733]\t Training Loss 2.7306\t Accuracy 0.4475\n",
      "Epoch [0][5]\t Batch [360][733]\t Training Loss 2.6971\t Accuracy 0.4504\n",
      "Epoch [0][5]\t Batch [370][733]\t Training Loss 2.6637\t Accuracy 0.4530\n",
      "Epoch [0][5]\t Batch [380][733]\t Training Loss 2.6289\t Accuracy 0.4560\n",
      "Epoch [0][5]\t Batch [390][733]\t Training Loss 2.5887\t Accuracy 0.4611\n",
      "Epoch [0][5]\t Batch [400][733]\t Training Loss 2.5562\t Accuracy 0.4645\n",
      "Epoch [0][5]\t Batch [410][733]\t Training Loss 2.5242\t Accuracy 0.4676\n",
      "Epoch [0][5]\t Batch [420][733]\t Training Loss 2.4950\t Accuracy 0.4705\n",
      "Epoch [0][5]\t Batch [430][733]\t Training Loss 2.4646\t Accuracy 0.4738\n",
      "Epoch [0][5]\t Batch [440][733]\t Training Loss 2.4380\t Accuracy 0.4757\n",
      "Epoch [0][5]\t Batch [450][733]\t Training Loss 2.4094\t Accuracy 0.4786\n",
      "Epoch [0][5]\t Batch [460][733]\t Training Loss 2.3809\t Accuracy 0.4822\n",
      "Epoch [0][5]\t Batch [470][733]\t Training Loss 2.3563\t Accuracy 0.4845\n",
      "Epoch [0][5]\t Batch [480][733]\t Training Loss 2.3317\t Accuracy 0.4872\n",
      "Epoch [0][5]\t Batch [490][733]\t Training Loss 2.3099\t Accuracy 0.4892\n",
      "Epoch [0][5]\t Batch [500][733]\t Training Loss 2.2878\t Accuracy 0.4914\n",
      "Epoch [0][5]\t Batch [510][733]\t Training Loss 2.2672\t Accuracy 0.4936\n",
      "Epoch [0][5]\t Batch [520][733]\t Training Loss 2.2432\t Accuracy 0.4966\n",
      "Epoch [0][5]\t Batch [530][733]\t Training Loss 2.2233\t Accuracy 0.4988\n",
      "Epoch [0][5]\t Batch [540][733]\t Training Loss 2.2044\t Accuracy 0.5006\n",
      "Epoch [0][5]\t Batch [550][733]\t Training Loss 2.1848\t Accuracy 0.5034\n",
      "Epoch [0][5]\t Batch [560][733]\t Training Loss 2.1662\t Accuracy 0.5054\n",
      "Epoch [0][5]\t Batch [570][733]\t Training Loss 2.1491\t Accuracy 0.5073\n",
      "Epoch [0][5]\t Batch [580][733]\t Training Loss 2.1303\t Accuracy 0.5100\n",
      "Epoch [0][5]\t Batch [590][733]\t Training Loss 2.1130\t Accuracy 0.5121\n",
      "Epoch [0][5]\t Batch [600][733]\t Training Loss 2.0979\t Accuracy 0.5137\n",
      "Epoch [0][5]\t Batch [610][733]\t Training Loss 2.0828\t Accuracy 0.5157\n",
      "Epoch [0][5]\t Batch [620][733]\t Training Loss 2.0665\t Accuracy 0.5177\n",
      "Epoch [0][5]\t Batch [630][733]\t Training Loss 2.0523\t Accuracy 0.5196\n",
      "Epoch [0][5]\t Batch [640][733]\t Training Loss 2.0370\t Accuracy 0.5217\n",
      "Epoch [0][5]\t Batch [650][733]\t Training Loss 2.0210\t Accuracy 0.5240\n",
      "Epoch [0][5]\t Batch [660][733]\t Training Loss 2.0057\t Accuracy 0.5263\n",
      "Epoch [0][5]\t Batch [670][733]\t Training Loss 1.9926\t Accuracy 0.5279\n",
      "Epoch [0][5]\t Batch [680][733]\t Training Loss 1.9782\t Accuracy 0.5297\n",
      "Epoch [0][5]\t Batch [690][733]\t Training Loss 1.9647\t Accuracy 0.5315\n",
      "Epoch [0][5]\t Batch [700][733]\t Training Loss 1.9507\t Accuracy 0.5333\n",
      "Epoch [0][5]\t Batch [710][733]\t Training Loss 1.9359\t Accuracy 0.5358\n",
      "Epoch [0][5]\t Batch [720][733]\t Training Loss 1.9207\t Accuracy 0.5385\n",
      "Epoch [0][5]\t Batch [730][733]\t Training Loss 1.9084\t Accuracy 0.5402\n",
      "\n",
      "Epoch [0]\t Average training loss 1.9061\t Average training accuracy 0.5405\n",
      "Epoch [0]\t Average validation loss 1.2199\t Average validation accuracy 0.5869\n",
      "\n",
      "Epoch [1][5]\t Batch [0][733]\t Training Loss 1.0030\t Accuracy 0.6933\n",
      "Epoch [1][5]\t Batch [10][733]\t Training Loss 1.0680\t Accuracy 0.6630\n",
      "Epoch [1][5]\t Batch [20][733]\t Training Loss 1.0482\t Accuracy 0.6590\n",
      "Epoch [1][5]\t Batch [30][733]\t Training Loss 1.0220\t Accuracy 0.6641\n",
      "Epoch [1][5]\t Batch [40][733]\t Training Loss 1.0302\t Accuracy 0.6615\n",
      "Epoch [1][5]\t Batch [50][733]\t Training Loss 1.0330\t Accuracy 0.6601\n",
      "Epoch [1][5]\t Batch [60][733]\t Training Loss 1.0466\t Accuracy 0.6555\n",
      "Epoch [1][5]\t Batch [70][733]\t Training Loss 1.0322\t Accuracy 0.6605\n",
      "Epoch [1][5]\t Batch [80][733]\t Training Loss 1.0399\t Accuracy 0.6578\n",
      "Epoch [1][5]\t Batch [90][733]\t Training Loss 1.0447\t Accuracy 0.6558\n",
      "Epoch [1][5]\t Batch [100][733]\t Training Loss 1.0404\t Accuracy 0.6582\n",
      "Epoch [1][5]\t Batch [110][733]\t Training Loss 1.0305\t Accuracy 0.6615\n",
      "Epoch [1][5]\t Batch [120][733]\t Training Loss 1.0307\t Accuracy 0.6618\n",
      "Epoch [1][5]\t Batch [130][733]\t Training Loss 1.0361\t Accuracy 0.6588\n",
      "Epoch [1][5]\t Batch [140][733]\t Training Loss 1.0335\t Accuracy 0.6601\n",
      "Epoch [1][5]\t Batch [150][733]\t Training Loss 1.0337\t Accuracy 0.6594\n",
      "Epoch [1][5]\t Batch [160][733]\t Training Loss 1.0351\t Accuracy 0.6603\n",
      "Epoch [1][5]\t Batch [170][733]\t Training Loss 1.0336\t Accuracy 0.6606\n",
      "Epoch [1][5]\t Batch [180][733]\t Training Loss 1.0330\t Accuracy 0.6608\n",
      "Epoch [1][5]\t Batch [190][733]\t Training Loss 1.0335\t Accuracy 0.6605\n",
      "Epoch [1][5]\t Batch [200][733]\t Training Loss 1.0315\t Accuracy 0.6612\n",
      "Epoch [1][5]\t Batch [210][733]\t Training Loss 1.0284\t Accuracy 0.6625\n",
      "Epoch [1][5]\t Batch [220][733]\t Training Loss 1.0281\t Accuracy 0.6622\n",
      "Epoch [1][5]\t Batch [230][733]\t Training Loss 1.0271\t Accuracy 0.6633\n",
      "Epoch [1][5]\t Batch [240][733]\t Training Loss 1.0237\t Accuracy 0.6646\n",
      "Epoch [1][5]\t Batch [250][733]\t Training Loss 1.0235\t Accuracy 0.6646\n",
      "Epoch [1][5]\t Batch [260][733]\t Training Loss 1.0203\t Accuracy 0.6644\n",
      "Epoch [1][5]\t Batch [270][733]\t Training Loss 1.0148\t Accuracy 0.6664\n",
      "Epoch [1][5]\t Batch [280][733]\t Training Loss 1.0106\t Accuracy 0.6681\n",
      "Epoch [1][5]\t Batch [290][733]\t Training Loss 1.0078\t Accuracy 0.6690\n",
      "Epoch [1][5]\t Batch [300][733]\t Training Loss 1.0074\t Accuracy 0.6694\n",
      "Epoch [1][5]\t Batch [310][733]\t Training Loss 1.0071\t Accuracy 0.6700\n",
      "Epoch [1][5]\t Batch [320][733]\t Training Loss 1.0072\t Accuracy 0.6702\n",
      "Epoch [1][5]\t Batch [330][733]\t Training Loss 1.0054\t Accuracy 0.6705\n",
      "Epoch [1][5]\t Batch [340][733]\t Training Loss 1.0061\t Accuracy 0.6701\n",
      "Epoch [1][5]\t Batch [350][733]\t Training Loss 1.0064\t Accuracy 0.6699\n",
      "Epoch [1][5]\t Batch [360][733]\t Training Loss 1.0042\t Accuracy 0.6706\n",
      "Epoch [1][5]\t Batch [370][733]\t Training Loss 1.0036\t Accuracy 0.6708\n",
      "Epoch [1][5]\t Batch [380][733]\t Training Loss 1.0024\t Accuracy 0.6712\n",
      "Epoch [1][5]\t Batch [390][733]\t Training Loss 0.9997\t Accuracy 0.6718\n",
      "Epoch [1][5]\t Batch [400][733]\t Training Loss 0.9961\t Accuracy 0.6732\n",
      "Epoch [1][5]\t Batch [410][733]\t Training Loss 0.9949\t Accuracy 0.6738\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1][5]\t Batch [420][733]\t Training Loss 0.9930\t Accuracy 0.6743\n",
      "Epoch [1][5]\t Batch [430][733]\t Training Loss 0.9914\t Accuracy 0.6747\n",
      "Epoch [1][5]\t Batch [440][733]\t Training Loss 0.9903\t Accuracy 0.6749\n",
      "Epoch [1][5]\t Batch [450][733]\t Training Loss 0.9894\t Accuracy 0.6754\n",
      "Epoch [1][5]\t Batch [460][733]\t Training Loss 0.9875\t Accuracy 0.6758\n",
      "Epoch [1][5]\t Batch [470][733]\t Training Loss 0.9860\t Accuracy 0.6763\n",
      "Epoch [1][5]\t Batch [480][733]\t Training Loss 0.9840\t Accuracy 0.6771\n",
      "Epoch [1][5]\t Batch [490][733]\t Training Loss 0.9832\t Accuracy 0.6774\n",
      "Epoch [1][5]\t Batch [500][733]\t Training Loss 0.9821\t Accuracy 0.6777\n",
      "Epoch [1][5]\t Batch [510][733]\t Training Loss 0.9794\t Accuracy 0.6785\n",
      "Epoch [1][5]\t Batch [520][733]\t Training Loss 0.9796\t Accuracy 0.6782\n",
      "Epoch [1][5]\t Batch [530][733]\t Training Loss 0.9782\t Accuracy 0.6788\n",
      "Epoch [1][5]\t Batch [540][733]\t Training Loss 0.9753\t Accuracy 0.6801\n",
      "Epoch [1][5]\t Batch [550][733]\t Training Loss 0.9743\t Accuracy 0.6806\n",
      "Epoch [1][5]\t Batch [560][733]\t Training Loss 0.9719\t Accuracy 0.6808\n",
      "Epoch [1][5]\t Batch [570][733]\t Training Loss 0.9698\t Accuracy 0.6813\n",
      "Epoch [1][5]\t Batch [580][733]\t Training Loss 0.9677\t Accuracy 0.6819\n",
      "Epoch [1][5]\t Batch [590][733]\t Training Loss 0.9672\t Accuracy 0.6820\n",
      "Epoch [1][5]\t Batch [600][733]\t Training Loss 0.9671\t Accuracy 0.6818\n",
      "Epoch [1][5]\t Batch [610][733]\t Training Loss 0.9679\t Accuracy 0.6815\n",
      "Epoch [1][5]\t Batch [620][733]\t Training Loss 0.9666\t Accuracy 0.6820\n",
      "Epoch [1][5]\t Batch [630][733]\t Training Loss 0.9646\t Accuracy 0.6826\n",
      "Epoch [1][5]\t Batch [640][733]\t Training Loss 0.9639\t Accuracy 0.6830\n",
      "Epoch [1][5]\t Batch [650][733]\t Training Loss 0.9637\t Accuracy 0.6833\n",
      "Epoch [1][5]\t Batch [660][733]\t Training Loss 0.9626\t Accuracy 0.6835\n",
      "Epoch [1][5]\t Batch [670][733]\t Training Loss 0.9629\t Accuracy 0.6836\n",
      "Epoch [1][5]\t Batch [680][733]\t Training Loss 0.9623\t Accuracy 0.6838\n",
      "Epoch [1][5]\t Batch [690][733]\t Training Loss 0.9604\t Accuracy 0.6844\n",
      "Epoch [1][5]\t Batch [700][733]\t Training Loss 0.9598\t Accuracy 0.6846\n",
      "Epoch [1][5]\t Batch [710][733]\t Training Loss 0.9591\t Accuracy 0.6849\n",
      "Epoch [1][5]\t Batch [720][733]\t Training Loss 0.9580\t Accuracy 0.6851\n",
      "Epoch [1][5]\t Batch [730][733]\t Training Loss 0.9573\t Accuracy 0.6852\n",
      "\n",
      "Epoch [1]\t Average training loss 0.9572\t Average training accuracy 0.6854\n",
      "Epoch [1]\t Average validation loss 0.8542\t Average validation accuracy 0.7059\n",
      "\n",
      "Epoch [2][5]\t Batch [0][733]\t Training Loss 1.0620\t Accuracy 0.7200\n",
      "Epoch [2][5]\t Batch [10][733]\t Training Loss 1.0808\t Accuracy 0.6606\n",
      "Epoch [2][5]\t Batch [20][733]\t Training Loss 0.9561\t Accuracy 0.6933\n",
      "Epoch [2][5]\t Batch [30][733]\t Training Loss 0.9591\t Accuracy 0.6873\n",
      "Epoch [2][5]\t Batch [40][733]\t Training Loss 0.9331\t Accuracy 0.6920\n",
      "Epoch [2][5]\t Batch [50][733]\t Training Loss 0.9236\t Accuracy 0.6954\n",
      "Epoch [2][5]\t Batch [60][733]\t Training Loss 0.9305\t Accuracy 0.6898\n",
      "Epoch [2][5]\t Batch [70][733]\t Training Loss 0.9281\t Accuracy 0.6924\n",
      "Epoch [2][5]\t Batch [80][733]\t Training Loss 0.9216\t Accuracy 0.6945\n",
      "Epoch [2][5]\t Batch [90][733]\t Training Loss 0.9169\t Accuracy 0.6958\n",
      "Epoch [2][5]\t Batch [100][733]\t Training Loss 0.9055\t Accuracy 0.7018\n",
      "Epoch [2][5]\t Batch [110][733]\t Training Loss 0.9082\t Accuracy 0.7008\n",
      "Epoch [2][5]\t Batch [120][733]\t Training Loss 0.9069\t Accuracy 0.6987\n",
      "Epoch [2][5]\t Batch [130][733]\t Training Loss 0.9065\t Accuracy 0.6984\n",
      "Epoch [2][5]\t Batch [140][733]\t Training Loss 0.9070\t Accuracy 0.6986\n",
      "Epoch [2][5]\t Batch [150][733]\t Training Loss 0.9064\t Accuracy 0.6994\n",
      "Epoch [2][5]\t Batch [160][733]\t Training Loss 0.9009\t Accuracy 0.7015\n",
      "Epoch [2][5]\t Batch [170][733]\t Training Loss 0.8982\t Accuracy 0.7032\n",
      "Epoch [2][5]\t Batch [180][733]\t Training Loss 0.8902\t Accuracy 0.7056\n",
      "Epoch [2][5]\t Batch [190][733]\t Training Loss 0.8825\t Accuracy 0.7080\n",
      "Epoch [2][5]\t Batch [200][733]\t Training Loss 0.8821\t Accuracy 0.7091\n",
      "Epoch [2][5]\t Batch [210][733]\t Training Loss 0.8808\t Accuracy 0.7084\n",
      "Epoch [2][5]\t Batch [220][733]\t Training Loss 0.8826\t Accuracy 0.7084\n",
      "Epoch [2][5]\t Batch [230][733]\t Training Loss 0.8788\t Accuracy 0.7095\n",
      "Epoch [2][5]\t Batch [240][733]\t Training Loss 0.8767\t Accuracy 0.7093\n",
      "Epoch [2][5]\t Batch [250][733]\t Training Loss 0.8736\t Accuracy 0.7100\n",
      "Epoch [2][5]\t Batch [260][733]\t Training Loss 0.8717\t Accuracy 0.7112\n",
      "Epoch [2][5]\t Batch [270][733]\t Training Loss 0.8718\t Accuracy 0.7109\n",
      "Epoch [2][5]\t Batch [280][733]\t Training Loss 0.8714\t Accuracy 0.7113\n",
      "Epoch [2][5]\t Batch [290][733]\t Training Loss 0.8694\t Accuracy 0.7115\n",
      "Epoch [2][5]\t Batch [300][733]\t Training Loss 0.8687\t Accuracy 0.7119\n",
      "Epoch [2][5]\t Batch [310][733]\t Training Loss 0.8651\t Accuracy 0.7129\n",
      "Epoch [2][5]\t Batch [320][733]\t Training Loss 0.8651\t Accuracy 0.7132\n",
      "Epoch [2][5]\t Batch [330][733]\t Training Loss 0.8631\t Accuracy 0.7138\n",
      "Epoch [2][5]\t Batch [340][733]\t Training Loss 0.8595\t Accuracy 0.7150\n",
      "Epoch [2][5]\t Batch [350][733]\t Training Loss 0.8584\t Accuracy 0.7149\n",
      "Epoch [2][5]\t Batch [360][733]\t Training Loss 0.8568\t Accuracy 0.7155\n",
      "Epoch [2][5]\t Batch [370][733]\t Training Loss 0.8577\t Accuracy 0.7150\n",
      "Epoch [2][5]\t Batch [380][733]\t Training Loss 0.8579\t Accuracy 0.7151\n",
      "Epoch [2][5]\t Batch [390][733]\t Training Loss 0.8582\t Accuracy 0.7156\n",
      "Epoch [2][5]\t Batch [400][733]\t Training Loss 0.8553\t Accuracy 0.7164\n",
      "Epoch [2][5]\t Batch [410][733]\t Training Loss 0.8539\t Accuracy 0.7166\n",
      "Epoch [2][5]\t Batch [420][733]\t Training Loss 0.8534\t Accuracy 0.7169\n",
      "Epoch [2][5]\t Batch [430][733]\t Training Loss 0.8526\t Accuracy 0.7173\n",
      "Epoch [2][5]\t Batch [440][733]\t Training Loss 0.8516\t Accuracy 0.7178\n",
      "Epoch [2][5]\t Batch [450][733]\t Training Loss 0.8492\t Accuracy 0.7184\n",
      "Epoch [2][5]\t Batch [460][733]\t Training Loss 0.8481\t Accuracy 0.7186\n",
      "Epoch [2][5]\t Batch [470][733]\t Training Loss 0.8479\t Accuracy 0.7186\n",
      "Epoch [2][5]\t Batch [480][733]\t Training Loss 0.8465\t Accuracy 0.7191\n",
      "Epoch [2][5]\t Batch [490][733]\t Training Loss 0.8454\t Accuracy 0.7193\n",
      "Epoch [2][5]\t Batch [500][733]\t Training Loss 0.8453\t Accuracy 0.7194\n",
      "Epoch [2][5]\t Batch [510][733]\t Training Loss 0.8440\t Accuracy 0.7201\n",
      "Epoch [2][5]\t Batch [520][733]\t Training Loss 0.8439\t Accuracy 0.7203\n",
      "Epoch [2][5]\t Batch [530][733]\t Training Loss 0.8446\t Accuracy 0.7202\n",
      "Epoch [2][5]\t Batch [540][733]\t Training Loss 0.8449\t Accuracy 0.7203\n",
      "Epoch [2][5]\t Batch [550][733]\t Training Loss 0.8453\t Accuracy 0.7206\n",
      "Epoch [2][5]\t Batch [560][733]\t Training Loss 0.8455\t Accuracy 0.7207\n",
      "Epoch [2][5]\t Batch [570][733]\t Training Loss 0.8443\t Accuracy 0.7211\n",
      "Epoch [2][5]\t Batch [580][733]\t Training Loss 0.8421\t Accuracy 0.7219\n",
      "Epoch [2][5]\t Batch [590][733]\t Training Loss 0.8423\t Accuracy 0.7222\n",
      "Epoch [2][5]\t Batch [600][733]\t Training Loss 0.8418\t Accuracy 0.7222\n",
      "Epoch [2][5]\t Batch [610][733]\t Training Loss 0.8412\t Accuracy 0.7226\n",
      "Epoch [2][5]\t Batch [620][733]\t Training Loss 0.8409\t Accuracy 0.7223\n",
      "Epoch [2][5]\t Batch [630][733]\t Training Loss 0.8414\t Accuracy 0.7223\n",
      "Epoch [2][5]\t Batch [640][733]\t Training Loss 0.8399\t Accuracy 0.7228\n",
      "Epoch [2][5]\t Batch [650][733]\t Training Loss 0.8396\t Accuracy 0.7227\n",
      "Epoch [2][5]\t Batch [660][733]\t Training Loss 0.8395\t Accuracy 0.7228\n",
      "Epoch [2][5]\t Batch [670][733]\t Training Loss 0.8391\t Accuracy 0.7230\n",
      "Epoch [2][5]\t Batch [680][733]\t Training Loss 0.8371\t Accuracy 0.7235\n",
      "Epoch [2][5]\t Batch [690][733]\t Training Loss 0.8373\t Accuracy 0.7236\n",
      "Epoch [2][5]\t Batch [700][733]\t Training Loss 0.8388\t Accuracy 0.7231\n",
      "Epoch [2][5]\t Batch [710][733]\t Training Loss 0.8395\t Accuracy 0.7232\n",
      "Epoch [2][5]\t Batch [720][733]\t Training Loss 0.8402\t Accuracy 0.7228\n",
      "Epoch [2][5]\t Batch [730][733]\t Training Loss 0.8397\t Accuracy 0.7231\n",
      "\n",
      "Epoch [2]\t Average training loss 0.8396\t Average training accuracy 0.7231\n",
      "Epoch [2]\t Average validation loss 0.7666\t Average validation accuracy 0.7406\n",
      "\n",
      "Epoch [3][5]\t Batch [0][733]\t Training Loss 0.7342\t Accuracy 0.7733\n",
      "Epoch [3][5]\t Batch [10][733]\t Training Loss 0.8357\t Accuracy 0.7285\n",
      "Epoch [3][5]\t Batch [20][733]\t Training Loss 0.8139\t Accuracy 0.7390\n",
      "Epoch [3][5]\t Batch [30][733]\t Training Loss 0.8220\t Accuracy 0.7351\n",
      "Epoch [3][5]\t Batch [40][733]\t Training Loss 0.8029\t Accuracy 0.7389\n",
      "Epoch [3][5]\t Batch [50][733]\t Training Loss 0.8175\t Accuracy 0.7370\n",
      "Epoch [3][5]\t Batch [60][733]\t Training Loss 0.8215\t Accuracy 0.7338\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3][5]\t Batch [70][733]\t Training Loss 0.8275\t Accuracy 0.7298\n",
      "Epoch [3][5]\t Batch [80][733]\t Training Loss 0.8275\t Accuracy 0.7297\n",
      "Epoch [3][5]\t Batch [90][733]\t Training Loss 0.8196\t Accuracy 0.7311\n",
      "Epoch [3][5]\t Batch [100][733]\t Training Loss 0.8199\t Accuracy 0.7306\n",
      "Epoch [3][5]\t Batch [110][733]\t Training Loss 0.8193\t Accuracy 0.7317\n",
      "Epoch [3][5]\t Batch [120][733]\t Training Loss 0.8147\t Accuracy 0.7330\n",
      "Epoch [3][5]\t Batch [130][733]\t Training Loss 0.8125\t Accuracy 0.7330\n",
      "Epoch [3][5]\t Batch [140][733]\t Training Loss 0.8096\t Accuracy 0.7334\n",
      "Epoch [3][5]\t Batch [150][733]\t Training Loss 0.8061\t Accuracy 0.7342\n",
      "Epoch [3][5]\t Batch [160][733]\t Training Loss 0.7987\t Accuracy 0.7368\n",
      "Epoch [3][5]\t Batch [170][733]\t Training Loss 0.7966\t Accuracy 0.7372\n",
      "Epoch [3][5]\t Batch [180][733]\t Training Loss 0.7939\t Accuracy 0.7386\n",
      "Epoch [3][5]\t Batch [190][733]\t Training Loss 0.7932\t Accuracy 0.7393\n",
      "Epoch [3][5]\t Batch [200][733]\t Training Loss 0.7910\t Accuracy 0.7398\n",
      "Epoch [3][5]\t Batch [210][733]\t Training Loss 0.7925\t Accuracy 0.7394\n",
      "Epoch [3][5]\t Batch [220][733]\t Training Loss 0.7923\t Accuracy 0.7401\n",
      "Epoch [3][5]\t Batch [230][733]\t Training Loss 0.7899\t Accuracy 0.7406\n",
      "Epoch [3][5]\t Batch [240][733]\t Training Loss 0.7926\t Accuracy 0.7399\n",
      "Epoch [3][5]\t Batch [250][733]\t Training Loss 0.7911\t Accuracy 0.7398\n",
      "Epoch [3][5]\t Batch [260][733]\t Training Loss 0.7900\t Accuracy 0.7395\n",
      "Epoch [3][5]\t Batch [270][733]\t Training Loss 0.7866\t Accuracy 0.7400\n",
      "Epoch [3][5]\t Batch [280][733]\t Training Loss 0.7849\t Accuracy 0.7402\n",
      "Epoch [3][5]\t Batch [290][733]\t Training Loss 0.7859\t Accuracy 0.7401\n",
      "Epoch [3][5]\t Batch [300][733]\t Training Loss 0.7848\t Accuracy 0.7410\n",
      "Epoch [3][5]\t Batch [310][733]\t Training Loss 0.7843\t Accuracy 0.7408\n",
      "Epoch [3][5]\t Batch [320][733]\t Training Loss 0.7857\t Accuracy 0.7396\n",
      "Epoch [3][5]\t Batch [330][733]\t Training Loss 0.7825\t Accuracy 0.7402\n",
      "Epoch [3][5]\t Batch [340][733]\t Training Loss 0.7818\t Accuracy 0.7403\n",
      "Epoch [3][5]\t Batch [350][733]\t Training Loss 0.7833\t Accuracy 0.7398\n",
      "Epoch [3][5]\t Batch [360][733]\t Training Loss 0.7838\t Accuracy 0.7396\n",
      "Epoch [3][5]\t Batch [370][733]\t Training Loss 0.7820\t Accuracy 0.7398\n",
      "Epoch [3][5]\t Batch [380][733]\t Training Loss 0.7841\t Accuracy 0.7394\n",
      "Epoch [3][5]\t Batch [390][733]\t Training Loss 0.7836\t Accuracy 0.7399\n",
      "Epoch [3][5]\t Batch [400][733]\t Training Loss 0.7834\t Accuracy 0.7403\n",
      "Epoch [3][5]\t Batch [410][733]\t Training Loss 0.7828\t Accuracy 0.7408\n",
      "Epoch [3][5]\t Batch [420][733]\t Training Loss 0.7826\t Accuracy 0.7411\n",
      "Epoch [3][5]\t Batch [430][733]\t Training Loss 0.7820\t Accuracy 0.7415\n",
      "Epoch [3][5]\t Batch [440][733]\t Training Loss 0.7818\t Accuracy 0.7415\n",
      "Epoch [3][5]\t Batch [450][733]\t Training Loss 0.7808\t Accuracy 0.7414\n",
      "Epoch [3][5]\t Batch [460][733]\t Training Loss 0.7818\t Accuracy 0.7413\n",
      "Epoch [3][5]\t Batch [470][733]\t Training Loss 0.7831\t Accuracy 0.7406\n",
      "Epoch [3][5]\t Batch [480][733]\t Training Loss 0.7823\t Accuracy 0.7407\n",
      "Epoch [3][5]\t Batch [490][733]\t Training Loss 0.7815\t Accuracy 0.7412\n",
      "Epoch [3][5]\t Batch [500][733]\t Training Loss 0.7799\t Accuracy 0.7418\n",
      "Epoch [3][5]\t Batch [510][733]\t Training Loss 0.7800\t Accuracy 0.7417\n",
      "Epoch [3][5]\t Batch [520][733]\t Training Loss 0.7796\t Accuracy 0.7419\n",
      "Epoch [3][5]\t Batch [530][733]\t Training Loss 0.7792\t Accuracy 0.7421\n",
      "Epoch [3][5]\t Batch [540][733]\t Training Loss 0.7795\t Accuracy 0.7420\n",
      "Epoch [3][5]\t Batch [550][733]\t Training Loss 0.7801\t Accuracy 0.7419\n",
      "Epoch [3][5]\t Batch [560][733]\t Training Loss 0.7799\t Accuracy 0.7417\n",
      "Epoch [3][5]\t Batch [570][733]\t Training Loss 0.7784\t Accuracy 0.7422\n",
      "Epoch [3][5]\t Batch [580][733]\t Training Loss 0.7773\t Accuracy 0.7423\n",
      "Epoch [3][5]\t Batch [590][733]\t Training Loss 0.7754\t Accuracy 0.7428\n",
      "Epoch [3][5]\t Batch [600][733]\t Training Loss 0.7758\t Accuracy 0.7427\n",
      "Epoch [3][5]\t Batch [610][733]\t Training Loss 0.7754\t Accuracy 0.7429\n",
      "Epoch [3][5]\t Batch [620][733]\t Training Loss 0.7745\t Accuracy 0.7432\n",
      "Epoch [3][5]\t Batch [630][733]\t Training Loss 0.7747\t Accuracy 0.7430\n",
      "Epoch [3][5]\t Batch [640][733]\t Training Loss 0.7737\t Accuracy 0.7432\n",
      "Epoch [3][5]\t Batch [650][733]\t Training Loss 0.7724\t Accuracy 0.7436\n",
      "Epoch [3][5]\t Batch [660][733]\t Training Loss 0.7722\t Accuracy 0.7439\n",
      "Epoch [3][5]\t Batch [670][733]\t Training Loss 0.7712\t Accuracy 0.7440\n",
      "Epoch [3][5]\t Batch [680][733]\t Training Loss 0.7712\t Accuracy 0.7439\n",
      "Epoch [3][5]\t Batch [690][733]\t Training Loss 0.7700\t Accuracy 0.7442\n",
      "Epoch [3][5]\t Batch [700][733]\t Training Loss 0.7701\t Accuracy 0.7443\n",
      "Epoch [3][5]\t Batch [710][733]\t Training Loss 0.7694\t Accuracy 0.7448\n",
      "Epoch [3][5]\t Batch [720][733]\t Training Loss 0.7687\t Accuracy 0.7453\n",
      "Epoch [3][5]\t Batch [730][733]\t Training Loss 0.7690\t Accuracy 0.7452\n",
      "\n",
      "Epoch [3]\t Average training loss 0.7694\t Average training accuracy 0.7452\n",
      "Epoch [3]\t Average validation loss 0.7010\t Average validation accuracy 0.7655\n",
      "\n",
      "Epoch [4][5]\t Batch [0][733]\t Training Loss 0.8736\t Accuracy 0.7200\n",
      "Epoch [4][5]\t Batch [10][733]\t Training Loss 0.7549\t Accuracy 0.7527\n",
      "Epoch [4][5]\t Batch [20][733]\t Training Loss 0.7248\t Accuracy 0.7600\n",
      "Epoch [4][5]\t Batch [30][733]\t Training Loss 0.7364\t Accuracy 0.7557\n",
      "Epoch [4][5]\t Batch [40][733]\t Training Loss 0.7559\t Accuracy 0.7476\n",
      "Epoch [4][5]\t Batch [50][733]\t Training Loss 0.7398\t Accuracy 0.7529\n",
      "Epoch [4][5]\t Batch [60][733]\t Training Loss 0.7444\t Accuracy 0.7510\n",
      "Epoch [4][5]\t Batch [70][733]\t Training Loss 0.7493\t Accuracy 0.7489\n",
      "Epoch [4][5]\t Batch [80][733]\t Training Loss 0.7461\t Accuracy 0.7500\n",
      "Epoch [4][5]\t Batch [90][733]\t Training Loss 0.7420\t Accuracy 0.7530\n",
      "Epoch [4][5]\t Batch [100][733]\t Training Loss 0.7344\t Accuracy 0.7551\n",
      "Epoch [4][5]\t Batch [110][733]\t Training Loss 0.7286\t Accuracy 0.7575\n",
      "Epoch [4][5]\t Batch [120][733]\t Training Loss 0.7335\t Accuracy 0.7566\n",
      "Epoch [4][5]\t Batch [130][733]\t Training Loss 0.7297\t Accuracy 0.7586\n",
      "Epoch [4][5]\t Batch [140][733]\t Training Loss 0.7283\t Accuracy 0.7584\n",
      "Epoch [4][5]\t Batch [150][733]\t Training Loss 0.7295\t Accuracy 0.7584\n",
      "Epoch [4][5]\t Batch [160][733]\t Training Loss 0.7269\t Accuracy 0.7603\n",
      "Epoch [4][5]\t Batch [170][733]\t Training Loss 0.7231\t Accuracy 0.7616\n",
      "Epoch [4][5]\t Batch [180][733]\t Training Loss 0.7264\t Accuracy 0.7616\n",
      "Epoch [4][5]\t Batch [190][733]\t Training Loss 0.7223\t Accuracy 0.7622\n",
      "Epoch [4][5]\t Batch [200][733]\t Training Loss 0.7225\t Accuracy 0.7627\n",
      "Epoch [4][5]\t Batch [210][733]\t Training Loss 0.7219\t Accuracy 0.7627\n",
      "Epoch [4][5]\t Batch [220][733]\t Training Loss 0.7203\t Accuracy 0.7630\n",
      "Epoch [4][5]\t Batch [230][733]\t Training Loss 0.7206\t Accuracy 0.7637\n",
      "Epoch [4][5]\t Batch [240][733]\t Training Loss 0.7199\t Accuracy 0.7639\n",
      "Epoch [4][5]\t Batch [250][733]\t Training Loss 0.7190\t Accuracy 0.7642\n",
      "Epoch [4][5]\t Batch [260][733]\t Training Loss 0.7207\t Accuracy 0.7632\n",
      "Epoch [4][5]\t Batch [270][733]\t Training Loss 0.7208\t Accuracy 0.7638\n",
      "Epoch [4][5]\t Batch [280][733]\t Training Loss 0.7190\t Accuracy 0.7648\n",
      "Epoch [4][5]\t Batch [290][733]\t Training Loss 0.7201\t Accuracy 0.7646\n",
      "Epoch [4][5]\t Batch [300][733]\t Training Loss 0.7197\t Accuracy 0.7646\n",
      "Epoch [4][5]\t Batch [310][733]\t Training Loss 0.7195\t Accuracy 0.7645\n",
      "Epoch [4][5]\t Batch [320][733]\t Training Loss 0.7185\t Accuracy 0.7649\n",
      "Epoch [4][5]\t Batch [330][733]\t Training Loss 0.7194\t Accuracy 0.7641\n",
      "Epoch [4][5]\t Batch [340][733]\t Training Loss 0.7201\t Accuracy 0.7639\n",
      "Epoch [4][5]\t Batch [350][733]\t Training Loss 0.7186\t Accuracy 0.7642\n",
      "Epoch [4][5]\t Batch [360][733]\t Training Loss 0.7209\t Accuracy 0.7633\n",
      "Epoch [4][5]\t Batch [370][733]\t Training Loss 0.7234\t Accuracy 0.7624\n",
      "Epoch [4][5]\t Batch [380][733]\t Training Loss 0.7232\t Accuracy 0.7623\n",
      "Epoch [4][5]\t Batch [390][733]\t Training Loss 0.7241\t Accuracy 0.7621\n",
      "Epoch [4][5]\t Batch [400][733]\t Training Loss 0.7247\t Accuracy 0.7621\n",
      "Epoch [4][5]\t Batch [410][733]\t Training Loss 0.7239\t Accuracy 0.7620\n",
      "Epoch [4][5]\t Batch [420][733]\t Training Loss 0.7253\t Accuracy 0.7614\n",
      "Epoch [4][5]\t Batch [430][733]\t Training Loss 0.7246\t Accuracy 0.7616\n",
      "Epoch [4][5]\t Batch [440][733]\t Training Loss 0.7252\t Accuracy 0.7615\n",
      "Epoch [4][5]\t Batch [450][733]\t Training Loss 0.7266\t Accuracy 0.7609\n",
      "Epoch [4][5]\t Batch [460][733]\t Training Loss 0.7259\t Accuracy 0.7610\n",
      "Epoch [4][5]\t Batch [470][733]\t Training Loss 0.7248\t Accuracy 0.7612\n",
      "Epoch [4][5]\t Batch [480][733]\t Training Loss 0.7240\t Accuracy 0.7614\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4][5]\t Batch [490][733]\t Training Loss 0.7238\t Accuracy 0.7615\n",
      "Epoch [4][5]\t Batch [500][733]\t Training Loss 0.7239\t Accuracy 0.7617\n",
      "Epoch [4][5]\t Batch [510][733]\t Training Loss 0.7234\t Accuracy 0.7618\n",
      "Epoch [4][5]\t Batch [520][733]\t Training Loss 0.7241\t Accuracy 0.7616\n",
      "Epoch [4][5]\t Batch [530][733]\t Training Loss 0.7231\t Accuracy 0.7617\n",
      "Epoch [4][5]\t Batch [540][733]\t Training Loss 0.7228\t Accuracy 0.7614\n",
      "Epoch [4][5]\t Batch [550][733]\t Training Loss 0.7229\t Accuracy 0.7615\n",
      "Epoch [4][5]\t Batch [560][733]\t Training Loss 0.7223\t Accuracy 0.7617\n",
      "Epoch [4][5]\t Batch [570][733]\t Training Loss 0.7231\t Accuracy 0.7614\n",
      "Epoch [4][5]\t Batch [580][733]\t Training Loss 0.7240\t Accuracy 0.7612\n",
      "Epoch [4][5]\t Batch [590][733]\t Training Loss 0.7226\t Accuracy 0.7617\n",
      "Epoch [4][5]\t Batch [600][733]\t Training Loss 0.7225\t Accuracy 0.7618\n",
      "Epoch [4][5]\t Batch [610][733]\t Training Loss 0.7237\t Accuracy 0.7614\n",
      "Epoch [4][5]\t Batch [620][733]\t Training Loss 0.7246\t Accuracy 0.7612\n",
      "Epoch [4][5]\t Batch [630][733]\t Training Loss 0.7242\t Accuracy 0.7615\n",
      "Epoch [4][5]\t Batch [640][733]\t Training Loss 0.7238\t Accuracy 0.7615\n",
      "Epoch [4][5]\t Batch [650][733]\t Training Loss 0.7246\t Accuracy 0.7610\n",
      "Epoch [4][5]\t Batch [660][733]\t Training Loss 0.7248\t Accuracy 0.7608\n",
      "Epoch [4][5]\t Batch [670][733]\t Training Loss 0.7242\t Accuracy 0.7614\n",
      "Epoch [4][5]\t Batch [680][733]\t Training Loss 0.7246\t Accuracy 0.7611\n",
      "Epoch [4][5]\t Batch [690][733]\t Training Loss 0.7250\t Accuracy 0.7610\n",
      "Epoch [4][5]\t Batch [700][733]\t Training Loss 0.7254\t Accuracy 0.7611\n",
      "Epoch [4][5]\t Batch [710][733]\t Training Loss 0.7254\t Accuracy 0.7614\n",
      "Epoch [4][5]\t Batch [720][733]\t Training Loss 0.7246\t Accuracy 0.7617\n",
      "Epoch [4][5]\t Batch [730][733]\t Training Loss 0.7241\t Accuracy 0.7619\n",
      "\n",
      "Epoch [4]\t Average training loss 0.7237\t Average training accuracy 0.7619\n",
      "Epoch [4]\t Average validation loss 0.6520\t Average validation accuracy 0.7865\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "convNet.is_training = True\n",
    "convNet, train_loss, train_acc, val_loss, val_acc = \\\n",
    "    train(convNet, criterion, sgd, dataset.train, dataset.validation, max_epoch, batch_size, disp_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing...\n",
      "The test accuracy is 0.7827.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "convNet.is_training = False\n",
    "test(convNet, criterion, dataset.test, batch_size, disp_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_loss' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [22]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m plot_loss_and_acc({\n\u001b[1;32m----> 2\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraining\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[43mtrain_loss\u001b[49m, train_acc], \n\u001b[0;32m      3\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mValidation\u001b[39m\u001b[38;5;124m'\u001b[39m: [val_loss, val_acc]\n\u001b[0;32m      4\u001b[0m })\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_loss' is not defined"
     ]
    }
   ],
   "source": [
    "plot_loss_and_acc({\n",
    "    'Training': [train_loss, train_acc], \n",
    "    'Validation': [val_loss, val_acc]\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from layers import FCLayer, ReLULayer, ConvLayer, MaxPoolingLayer, ReshapeLayer\n",
    "#Only run if you have to restart the kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Next, according to the requirements 4:**\n",
    "### **You need to implement the Dropout layer and train the network again.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0][15]\t Batch [0][550]\t Training Loss 8.6228\t Accuracy 0.1200\n",
      "Epoch [0][15]\t Batch [10][550]\t Training Loss 6.6525\t Accuracy 0.1136\n",
      "Epoch [0][15]\t Batch [20][550]\t Training Loss 6.0322\t Accuracy 0.1105\n",
      "Epoch [0][15]\t Batch [30][550]\t Training Loss 5.4694\t Accuracy 0.1148\n",
      "Epoch [0][15]\t Batch [40][550]\t Training Loss 5.1001\t Accuracy 0.1171\n",
      "Epoch [0][15]\t Batch [50][550]\t Training Loss 4.8137\t Accuracy 0.1218\n",
      "Epoch [0][15]\t Batch [60][550]\t Training Loss 4.5844\t Accuracy 0.1236\n",
      "Epoch [0][15]\t Batch [70][550]\t Training Loss 4.4094\t Accuracy 0.1242\n",
      "Epoch [0][15]\t Batch [80][550]\t Training Loss 4.2515\t Accuracy 0.1243\n",
      "Epoch [0][15]\t Batch [90][550]\t Training Loss 4.1203\t Accuracy 0.1275\n",
      "Epoch [0][15]\t Batch [100][550]\t Training Loss 4.0024\t Accuracy 0.1290\n",
      "Epoch [0][15]\t Batch [110][550]\t Training Loss 3.8965\t Accuracy 0.1295\n",
      "Epoch [0][15]\t Batch [120][550]\t Training Loss 3.8036\t Accuracy 0.1326\n",
      "Epoch [0][15]\t Batch [130][550]\t Training Loss 3.7232\t Accuracy 0.1342\n",
      "Epoch [0][15]\t Batch [140][550]\t Training Loss 3.6524\t Accuracy 0.1361\n",
      "Epoch [0][15]\t Batch [150][550]\t Training Loss 3.5862\t Accuracy 0.1378\n",
      "Epoch [0][15]\t Batch [160][550]\t Training Loss 3.5265\t Accuracy 0.1391\n",
      "Epoch [0][15]\t Batch [170][550]\t Training Loss 3.4768\t Accuracy 0.1397\n",
      "Epoch [0][15]\t Batch [180][550]\t Training Loss 3.4282\t Accuracy 0.1408\n",
      "Epoch [0][15]\t Batch [190][550]\t Training Loss 3.3817\t Accuracy 0.1412\n",
      "Epoch [0][15]\t Batch [200][550]\t Training Loss 3.3375\t Accuracy 0.1440\n",
      "Epoch [0][15]\t Batch [210][550]\t Training Loss 3.2969\t Accuracy 0.1453\n",
      "Epoch [0][15]\t Batch [220][550]\t Training Loss 3.2623\t Accuracy 0.1462\n",
      "Epoch [0][15]\t Batch [230][550]\t Training Loss 3.2261\t Accuracy 0.1477\n",
      "Epoch [0][15]\t Batch [240][550]\t Training Loss 3.1950\t Accuracy 0.1484\n",
      "Epoch [0][15]\t Batch [250][550]\t Training Loss 3.1620\t Accuracy 0.1492\n",
      "Epoch [0][15]\t Batch [260][550]\t Training Loss 3.1326\t Accuracy 0.1505\n",
      "Epoch [0][15]\t Batch [270][550]\t Training Loss 3.1074\t Accuracy 0.1518\n",
      "Epoch [0][15]\t Batch [280][550]\t Training Loss 3.0832\t Accuracy 0.1526\n",
      "Epoch [0][15]\t Batch [290][550]\t Training Loss 3.0596\t Accuracy 0.1532\n",
      "Epoch [0][15]\t Batch [300][550]\t Training Loss 3.0362\t Accuracy 0.1543\n",
      "Epoch [0][15]\t Batch [310][550]\t Training Loss 3.0159\t Accuracy 0.1552\n",
      "Epoch [0][15]\t Batch [320][550]\t Training Loss 2.9938\t Accuracy 0.1565\n",
      "Epoch [0][15]\t Batch [330][550]\t Training Loss 2.9750\t Accuracy 0.1575\n",
      "Epoch [0][15]\t Batch [340][550]\t Training Loss 2.9550\t Accuracy 0.1587\n",
      "Epoch [0][15]\t Batch [350][550]\t Training Loss 2.9367\t Accuracy 0.1595\n",
      "Epoch [0][15]\t Batch [360][550]\t Training Loss 2.9196\t Accuracy 0.1603\n",
      "Epoch [0][15]\t Batch [370][550]\t Training Loss 2.9031\t Accuracy 0.1612\n",
      "Epoch [0][15]\t Batch [380][550]\t Training Loss 2.8879\t Accuracy 0.1619\n",
      "Epoch [0][15]\t Batch [390][550]\t Training Loss 2.8736\t Accuracy 0.1623\n",
      "Epoch [0][15]\t Batch [400][550]\t Training Loss 2.8606\t Accuracy 0.1626\n",
      "Epoch [0][15]\t Batch [410][550]\t Training Loss 2.8472\t Accuracy 0.1632\n",
      "Epoch [0][15]\t Batch [420][550]\t Training Loss 2.8337\t Accuracy 0.1643\n",
      "Epoch [0][15]\t Batch [430][550]\t Training Loss 2.8201\t Accuracy 0.1654\n",
      "Epoch [0][15]\t Batch [440][550]\t Training Loss 2.8078\t Accuracy 0.1662\n",
      "Epoch [0][15]\t Batch [450][550]\t Training Loss 2.7957\t Accuracy 0.1668\n",
      "Epoch [0][15]\t Batch [460][550]\t Training Loss 2.7840\t Accuracy 0.1675\n",
      "Epoch [0][15]\t Batch [470][550]\t Training Loss 2.7746\t Accuracy 0.1680\n",
      "Epoch [0][15]\t Batch [480][550]\t Training Loss 2.7634\t Accuracy 0.1685\n",
      "Epoch [0][15]\t Batch [490][550]\t Training Loss 2.7544\t Accuracy 0.1692\n",
      "Epoch [0][15]\t Batch [500][550]\t Training Loss 2.7450\t Accuracy 0.1700\n",
      "Epoch [0][15]\t Batch [510][550]\t Training Loss 2.7354\t Accuracy 0.1705\n",
      "Epoch [0][15]\t Batch [520][550]\t Training Loss 2.7259\t Accuracy 0.1710\n",
      "Epoch [0][15]\t Batch [530][550]\t Training Loss 2.7160\t Accuracy 0.1716\n",
      "Epoch [0][15]\t Batch [540][550]\t Training Loss 2.7067\t Accuracy 0.1721\n",
      "\n",
      "Epoch [0]\t Average training loss 2.6997\t Average training accuracy 0.1725\n",
      "Epoch [0]\t Average validation loss 2.2463\t Average validation accuracy 0.2076\n",
      "\n",
      "Epoch [1][15]\t Batch [0][550]\t Training Loss 2.3050\t Accuracy 0.1800\n",
      "Epoch [1][15]\t Batch [10][550]\t Training Loss 2.2135\t Accuracy 0.1991\n",
      "Epoch [1][15]\t Batch [20][550]\t Training Loss 2.2491\t Accuracy 0.1986\n",
      "Epoch [1][15]\t Batch [30][550]\t Training Loss 2.2451\t Accuracy 0.2042\n",
      "Epoch [1][15]\t Batch [40][550]\t Training Loss 2.2411\t Accuracy 0.2034\n",
      "Epoch [1][15]\t Batch [50][550]\t Training Loss 2.2374\t Accuracy 0.2029\n",
      "Epoch [1][15]\t Batch [60][550]\t Training Loss 2.2360\t Accuracy 0.2044\n",
      "Epoch [1][15]\t Batch [70][550]\t Training Loss 2.2332\t Accuracy 0.2055\n",
      "Epoch [1][15]\t Batch [80][550]\t Training Loss 2.2299\t Accuracy 0.2053\n",
      "Epoch [1][15]\t Batch [90][550]\t Training Loss 2.2286\t Accuracy 0.2054\n",
      "Epoch [1][15]\t Batch [100][550]\t Training Loss 2.2281\t Accuracy 0.2044\n",
      "Epoch [1][15]\t Batch [110][550]\t Training Loss 2.2283\t Accuracy 0.2054\n",
      "Epoch [1][15]\t Batch [120][550]\t Training Loss 2.2270\t Accuracy 0.2057\n",
      "Epoch [1][15]\t Batch [130][550]\t Training Loss 2.2238\t Accuracy 0.2067\n",
      "Epoch [1][15]\t Batch [140][550]\t Training Loss 2.2216\t Accuracy 0.2065\n",
      "Epoch [1][15]\t Batch [150][550]\t Training Loss 2.2226\t Accuracy 0.2062\n",
      "Epoch [1][15]\t Batch [160][550]\t Training Loss 2.2218\t Accuracy 0.2073\n",
      "Epoch [1][15]\t Batch [170][550]\t Training Loss 2.2173\t Accuracy 0.2085\n",
      "Epoch [1][15]\t Batch [180][550]\t Training Loss 2.2150\t Accuracy 0.2092\n",
      "Epoch [1][15]\t Batch [190][550]\t Training Loss 2.2134\t Accuracy 0.2107\n",
      "Epoch [1][15]\t Batch [200][550]\t Training Loss 2.2136\t Accuracy 0.2104\n",
      "Epoch [1][15]\t Batch [210][550]\t Training Loss 2.2127\t Accuracy 0.2110\n",
      "Epoch [1][15]\t Batch [220][550]\t Training Loss 2.2109\t Accuracy 0.2113\n",
      "Epoch [1][15]\t Batch [230][550]\t Training Loss 2.2090\t Accuracy 0.2124\n",
      "Epoch [1][15]\t Batch [240][550]\t Training Loss 2.2100\t Accuracy 0.2132\n",
      "Epoch [1][15]\t Batch [250][550]\t Training Loss 2.2093\t Accuracy 0.2136\n",
      "Epoch [1][15]\t Batch [260][550]\t Training Loss 2.2074\t Accuracy 0.2146\n",
      "Epoch [1][15]\t Batch [270][550]\t Training Loss 2.2068\t Accuracy 0.2146\n",
      "Epoch [1][15]\t Batch [280][550]\t Training Loss 2.2077\t Accuracy 0.2142\n",
      "Epoch [1][15]\t Batch [290][550]\t Training Loss 2.2056\t Accuracy 0.2145\n",
      "Epoch [1][15]\t Batch [300][550]\t Training Loss 2.2046\t Accuracy 0.2144\n",
      "Epoch [1][15]\t Batch [310][550]\t Training Loss 2.2025\t Accuracy 0.2147\n",
      "Epoch [1][15]\t Batch [320][550]\t Training Loss 2.2008\t Accuracy 0.2149\n",
      "Epoch [1][15]\t Batch [330][550]\t Training Loss 2.1997\t Accuracy 0.2153\n",
      "Epoch [1][15]\t Batch [340][550]\t Training Loss 2.1974\t Accuracy 0.2161\n",
      "Epoch [1][15]\t Batch [350][550]\t Training Loss 2.1963\t Accuracy 0.2163\n",
      "Epoch [1][15]\t Batch [360][550]\t Training Loss 2.1946\t Accuracy 0.2167\n",
      "Epoch [1][15]\t Batch [370][550]\t Training Loss 2.1928\t Accuracy 0.2173\n",
      "Epoch [1][15]\t Batch [380][550]\t Training Loss 2.1925\t Accuracy 0.2173\n",
      "Epoch [1][15]\t Batch [390][550]\t Training Loss 2.1931\t Accuracy 0.2170\n",
      "Epoch [1][15]\t Batch [400][550]\t Training Loss 2.1917\t Accuracy 0.2177\n",
      "Epoch [1][15]\t Batch [410][550]\t Training Loss 2.1909\t Accuracy 0.2180\n",
      "Epoch [1][15]\t Batch [420][550]\t Training Loss 2.1893\t Accuracy 0.2186\n",
      "Epoch [1][15]\t Batch [430][550]\t Training Loss 2.1888\t Accuracy 0.2189\n",
      "Epoch [1][15]\t Batch [440][550]\t Training Loss 2.1881\t Accuracy 0.2190\n",
      "Epoch [1][15]\t Batch [450][550]\t Training Loss 2.1873\t Accuracy 0.2193\n",
      "Epoch [1][15]\t Batch [460][550]\t Training Loss 2.1861\t Accuracy 0.2192\n",
      "Epoch [1][15]\t Batch [470][550]\t Training Loss 2.1844\t Accuracy 0.2200\n",
      "Epoch [1][15]\t Batch [480][550]\t Training Loss 2.1830\t Accuracy 0.2206\n",
      "Epoch [1][15]\t Batch [490][550]\t Training Loss 2.1815\t Accuracy 0.2211\n",
      "Epoch [1][15]\t Batch [500][550]\t Training Loss 2.1805\t Accuracy 0.2215\n",
      "Epoch [1][15]\t Batch [510][550]\t Training Loss 2.1796\t Accuracy 0.2217\n",
      "Epoch [1][15]\t Batch [520][550]\t Training Loss 2.1781\t Accuracy 0.2221\n",
      "Epoch [1][15]\t Batch [530][550]\t Training Loss 2.1760\t Accuracy 0.2227\n",
      "Epoch [1][15]\t Batch [540][550]\t Training Loss 2.1746\t Accuracy 0.2232\n",
      "\n",
      "Epoch [1]\t Average training loss 2.1735\t Average training accuracy 0.2236\n",
      "Epoch [1]\t Average validation loss 2.1145\t Average validation accuracy 0.2474\n",
      "\n",
      "Epoch [2][15]\t Batch [0][550]\t Training Loss 2.2605\t Accuracy 0.1800\n",
      "Epoch [2][15]\t Batch [10][550]\t Training Loss 2.1149\t Accuracy 0.2336\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2][15]\t Batch [20][550]\t Training Loss 2.1083\t Accuracy 0.2467\n",
      "Epoch [2][15]\t Batch [30][550]\t Training Loss 2.0920\t Accuracy 0.2461\n",
      "Epoch [2][15]\t Batch [40][550]\t Training Loss 2.1051\t Accuracy 0.2454\n",
      "Epoch [2][15]\t Batch [50][550]\t Training Loss 2.1066\t Accuracy 0.2469\n",
      "Epoch [2][15]\t Batch [60][550]\t Training Loss 2.1047\t Accuracy 0.2469\n",
      "Epoch [2][15]\t Batch [70][550]\t Training Loss 2.1114\t Accuracy 0.2455\n",
      "Epoch [2][15]\t Batch [80][550]\t Training Loss 2.1144\t Accuracy 0.2448\n",
      "Epoch [2][15]\t Batch [90][550]\t Training Loss 2.1102\t Accuracy 0.2469\n",
      "Epoch [2][15]\t Batch [100][550]\t Training Loss 2.1094\t Accuracy 0.2466\n",
      "Epoch [2][15]\t Batch [110][550]\t Training Loss 2.1112\t Accuracy 0.2474\n",
      "Epoch [2][15]\t Batch [120][550]\t Training Loss 2.1063\t Accuracy 0.2478\n",
      "Epoch [2][15]\t Batch [130][550]\t Training Loss 2.1082\t Accuracy 0.2473\n",
      "Epoch [2][15]\t Batch [140][550]\t Training Loss 2.1114\t Accuracy 0.2462\n",
      "Epoch [2][15]\t Batch [150][550]\t Training Loss 2.1126\t Accuracy 0.2456\n",
      "Epoch [2][15]\t Batch [160][550]\t Training Loss 2.1165\t Accuracy 0.2442\n",
      "Epoch [2][15]\t Batch [170][550]\t Training Loss 2.1177\t Accuracy 0.2431\n",
      "Epoch [2][15]\t Batch [180][550]\t Training Loss 2.1156\t Accuracy 0.2440\n",
      "Epoch [2][15]\t Batch [190][550]\t Training Loss 2.1179\t Accuracy 0.2436\n",
      "Epoch [2][15]\t Batch [200][550]\t Training Loss 2.1178\t Accuracy 0.2440\n",
      "Epoch [2][15]\t Batch [210][550]\t Training Loss 2.1170\t Accuracy 0.2437\n",
      "Epoch [2][15]\t Batch [220][550]\t Training Loss 2.1158\t Accuracy 0.2440\n",
      "Epoch [2][15]\t Batch [230][550]\t Training Loss 2.1141\t Accuracy 0.2448\n",
      "Epoch [2][15]\t Batch [240][550]\t Training Loss 2.1123\t Accuracy 0.2449\n",
      "Epoch [2][15]\t Batch [250][550]\t Training Loss 2.1115\t Accuracy 0.2449\n",
      "Epoch [2][15]\t Batch [260][550]\t Training Loss 2.1100\t Accuracy 0.2454\n",
      "Epoch [2][15]\t Batch [270][550]\t Training Loss 2.1075\t Accuracy 0.2455\n",
      "Epoch [2][15]\t Batch [280][550]\t Training Loss 2.1058\t Accuracy 0.2462\n",
      "Epoch [2][15]\t Batch [290][550]\t Training Loss 2.1056\t Accuracy 0.2465\n",
      "Epoch [2][15]\t Batch [300][550]\t Training Loss 2.1028\t Accuracy 0.2474\n",
      "Epoch [2][15]\t Batch [310][550]\t Training Loss 2.1013\t Accuracy 0.2480\n",
      "Epoch [2][15]\t Batch [320][550]\t Training Loss 2.1009\t Accuracy 0.2489\n",
      "Epoch [2][15]\t Batch [330][550]\t Training Loss 2.1010\t Accuracy 0.2492\n",
      "Epoch [2][15]\t Batch [340][550]\t Training Loss 2.1011\t Accuracy 0.2490\n",
      "Epoch [2][15]\t Batch [350][550]\t Training Loss 2.1005\t Accuracy 0.2492\n",
      "Epoch [2][15]\t Batch [360][550]\t Training Loss 2.0995\t Accuracy 0.2495\n",
      "Epoch [2][15]\t Batch [370][550]\t Training Loss 2.0995\t Accuracy 0.2495\n",
      "Epoch [2][15]\t Batch [380][550]\t Training Loss 2.0999\t Accuracy 0.2493\n",
      "Epoch [2][15]\t Batch [390][550]\t Training Loss 2.0999\t Accuracy 0.2492\n",
      "Epoch [2][15]\t Batch [400][550]\t Training Loss 2.0995\t Accuracy 0.2493\n",
      "Epoch [2][15]\t Batch [410][550]\t Training Loss 2.0987\t Accuracy 0.2498\n",
      "Epoch [2][15]\t Batch [420][550]\t Training Loss 2.0987\t Accuracy 0.2501\n",
      "Epoch [2][15]\t Batch [430][550]\t Training Loss 2.0980\t Accuracy 0.2508\n",
      "Epoch [2][15]\t Batch [440][550]\t Training Loss 2.0969\t Accuracy 0.2511\n",
      "Epoch [2][15]\t Batch [450][550]\t Training Loss 2.0949\t Accuracy 0.2518\n",
      "Epoch [2][15]\t Batch [460][550]\t Training Loss 2.0941\t Accuracy 0.2520\n",
      "Epoch [2][15]\t Batch [470][550]\t Training Loss 2.0939\t Accuracy 0.2522\n",
      "Epoch [2][15]\t Batch [480][550]\t Training Loss 2.0938\t Accuracy 0.2519\n",
      "Epoch [2][15]\t Batch [490][550]\t Training Loss 2.0936\t Accuracy 0.2523\n",
      "Epoch [2][15]\t Batch [500][550]\t Training Loss 2.0929\t Accuracy 0.2528\n",
      "Epoch [2][15]\t Batch [510][550]\t Training Loss 2.0926\t Accuracy 0.2526\n",
      "Epoch [2][15]\t Batch [520][550]\t Training Loss 2.0923\t Accuracy 0.2526\n",
      "Epoch [2][15]\t Batch [530][550]\t Training Loss 2.0923\t Accuracy 0.2525\n",
      "Epoch [2][15]\t Batch [540][550]\t Training Loss 2.0916\t Accuracy 0.2530\n",
      "\n",
      "Epoch [2]\t Average training loss 2.0916\t Average training accuracy 0.2530\n",
      "Epoch [2]\t Average validation loss 2.0576\t Average validation accuracy 0.2644\n",
      "\n",
      "Epoch [3][15]\t Batch [0][550]\t Training Loss 2.0039\t Accuracy 0.2600\n",
      "Epoch [3][15]\t Batch [10][550]\t Training Loss 2.0353\t Accuracy 0.2745\n",
      "Epoch [3][15]\t Batch [20][550]\t Training Loss 2.0481\t Accuracy 0.2700\n",
      "Epoch [3][15]\t Batch [30][550]\t Training Loss 2.0578\t Accuracy 0.2632\n",
      "Epoch [3][15]\t Batch [40][550]\t Training Loss 2.0547\t Accuracy 0.2634\n",
      "Epoch [3][15]\t Batch [50][550]\t Training Loss 2.0501\t Accuracy 0.2655\n",
      "Epoch [3][15]\t Batch [60][550]\t Training Loss 2.0516\t Accuracy 0.2651\n",
      "Epoch [3][15]\t Batch [70][550]\t Training Loss 2.0522\t Accuracy 0.2642\n",
      "Epoch [3][15]\t Batch [80][550]\t Training Loss 2.0527\t Accuracy 0.2669\n",
      "Epoch [3][15]\t Batch [90][550]\t Training Loss 2.0504\t Accuracy 0.2674\n",
      "Epoch [3][15]\t Batch [100][550]\t Training Loss 2.0503\t Accuracy 0.2677\n",
      "Epoch [3][15]\t Batch [110][550]\t Training Loss 2.0507\t Accuracy 0.2672\n",
      "Epoch [3][15]\t Batch [120][550]\t Training Loss 2.0527\t Accuracy 0.2659\n",
      "Epoch [3][15]\t Batch [130][550]\t Training Loss 2.0545\t Accuracy 0.2647\n",
      "Epoch [3][15]\t Batch [140][550]\t Training Loss 2.0578\t Accuracy 0.2640\n",
      "Epoch [3][15]\t Batch [150][550]\t Training Loss 2.0577\t Accuracy 0.2647\n",
      "Epoch [3][15]\t Batch [160][550]\t Training Loss 2.0588\t Accuracy 0.2632\n",
      "Epoch [3][15]\t Batch [170][550]\t Training Loss 2.0576\t Accuracy 0.2634\n",
      "Epoch [3][15]\t Batch [180][550]\t Training Loss 2.0592\t Accuracy 0.2627\n",
      "Epoch [3][15]\t Batch [190][550]\t Training Loss 2.0572\t Accuracy 0.2649\n",
      "Epoch [3][15]\t Batch [200][550]\t Training Loss 2.0569\t Accuracy 0.2658\n",
      "Epoch [3][15]\t Batch [210][550]\t Training Loss 2.0560\t Accuracy 0.2652\n",
      "Epoch [3][15]\t Batch [220][550]\t Training Loss 2.0539\t Accuracy 0.2660\n",
      "Epoch [3][15]\t Batch [230][550]\t Training Loss 2.0557\t Accuracy 0.2665\n",
      "Epoch [3][15]\t Batch [240][550]\t Training Loss 2.0528\t Accuracy 0.2679\n",
      "Epoch [3][15]\t Batch [250][550]\t Training Loss 2.0536\t Accuracy 0.2673\n",
      "Epoch [3][15]\t Batch [260][550]\t Training Loss 2.0543\t Accuracy 0.2671\n",
      "Epoch [3][15]\t Batch [270][550]\t Training Loss 2.0508\t Accuracy 0.2686\n",
      "Epoch [3][15]\t Batch [280][550]\t Training Loss 2.0496\t Accuracy 0.2694\n",
      "Epoch [3][15]\t Batch [290][550]\t Training Loss 2.0483\t Accuracy 0.2697\n",
      "Epoch [3][15]\t Batch [300][550]\t Training Loss 2.0473\t Accuracy 0.2709\n",
      "Epoch [3][15]\t Batch [310][550]\t Training Loss 2.0459\t Accuracy 0.2710\n",
      "Epoch [3][15]\t Batch [320][550]\t Training Loss 2.0468\t Accuracy 0.2710\n",
      "Epoch [3][15]\t Batch [330][550]\t Training Loss 2.0463\t Accuracy 0.2707\n",
      "Epoch [3][15]\t Batch [340][550]\t Training Loss 2.0449\t Accuracy 0.2710\n",
      "Epoch [3][15]\t Batch [350][550]\t Training Loss 2.0446\t Accuracy 0.2711\n",
      "Epoch [3][15]\t Batch [360][550]\t Training Loss 2.0449\t Accuracy 0.2712\n",
      "Epoch [3][15]\t Batch [370][550]\t Training Loss 2.0437\t Accuracy 0.2719\n",
      "Epoch [3][15]\t Batch [380][550]\t Training Loss 2.0424\t Accuracy 0.2727\n",
      "Epoch [3][15]\t Batch [390][550]\t Training Loss 2.0437\t Accuracy 0.2722\n",
      "Epoch [3][15]\t Batch [400][550]\t Training Loss 2.0443\t Accuracy 0.2718\n",
      "Epoch [3][15]\t Batch [410][550]\t Training Loss 2.0447\t Accuracy 0.2717\n",
      "Epoch [3][15]\t Batch [420][550]\t Training Loss 2.0449\t Accuracy 0.2719\n",
      "Epoch [3][15]\t Batch [430][550]\t Training Loss 2.0453\t Accuracy 0.2715\n",
      "Epoch [3][15]\t Batch [440][550]\t Training Loss 2.0440\t Accuracy 0.2717\n",
      "Epoch [3][15]\t Batch [450][550]\t Training Loss 2.0441\t Accuracy 0.2714\n",
      "Epoch [3][15]\t Batch [460][550]\t Training Loss 2.0441\t Accuracy 0.2722\n",
      "Epoch [3][15]\t Batch [470][550]\t Training Loss 2.0440\t Accuracy 0.2721\n",
      "Epoch [3][15]\t Batch [480][550]\t Training Loss 2.0436\t Accuracy 0.2723\n",
      "Epoch [3][15]\t Batch [490][550]\t Training Loss 2.0428\t Accuracy 0.2730\n",
      "Epoch [3][15]\t Batch [500][550]\t Training Loss 2.0420\t Accuracy 0.2733\n",
      "Epoch [3][15]\t Batch [510][550]\t Training Loss 2.0408\t Accuracy 0.2738\n",
      "Epoch [3][15]\t Batch [520][550]\t Training Loss 2.0407\t Accuracy 0.2739\n",
      "Epoch [3][15]\t Batch [530][550]\t Training Loss 2.0403\t Accuracy 0.2741\n",
      "Epoch [3][15]\t Batch [540][550]\t Training Loss 2.0406\t Accuracy 0.2741\n",
      "\n",
      "Epoch [3]\t Average training loss 2.0398\t Average training accuracy 0.2747\n",
      "Epoch [3]\t Average validation loss 2.0163\t Average validation accuracy 0.2974\n",
      "\n",
      "Epoch [4][15]\t Batch [0][550]\t Training Loss 1.9956\t Accuracy 0.3500\n",
      "Epoch [4][15]\t Batch [10][550]\t Training Loss 1.9934\t Accuracy 0.3045\n",
      "Epoch [4][15]\t Batch [20][550]\t Training Loss 2.0036\t Accuracy 0.2919\n",
      "Epoch [4][15]\t Batch [30][550]\t Training Loss 2.0135\t Accuracy 0.2903\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4][15]\t Batch [40][550]\t Training Loss 2.0037\t Accuracy 0.2868\n",
      "Epoch [4][15]\t Batch [50][550]\t Training Loss 2.0089\t Accuracy 0.2835\n",
      "Epoch [4][15]\t Batch [60][550]\t Training Loss 2.0170\t Accuracy 0.2818\n",
      "Epoch [4][15]\t Batch [70][550]\t Training Loss 2.0159\t Accuracy 0.2831\n",
      "Epoch [4][15]\t Batch [80][550]\t Training Loss 2.0150\t Accuracy 0.2822\n",
      "Epoch [4][15]\t Batch [90][550]\t Training Loss 2.0148\t Accuracy 0.2838\n",
      "Epoch [4][15]\t Batch [100][550]\t Training Loss 2.0211\t Accuracy 0.2802\n",
      "Epoch [4][15]\t Batch [110][550]\t Training Loss 2.0258\t Accuracy 0.2798\n",
      "Epoch [4][15]\t Batch [120][550]\t Training Loss 2.0253\t Accuracy 0.2798\n",
      "Epoch [4][15]\t Batch [130][550]\t Training Loss 2.0240\t Accuracy 0.2804\n",
      "Epoch [4][15]\t Batch [140][550]\t Training Loss 2.0251\t Accuracy 0.2809\n",
      "Epoch [4][15]\t Batch [150][550]\t Training Loss 2.0232\t Accuracy 0.2815\n",
      "Epoch [4][15]\t Batch [160][550]\t Training Loss 2.0237\t Accuracy 0.2814\n",
      "Epoch [4][15]\t Batch [170][550]\t Training Loss 2.0224\t Accuracy 0.2822\n",
      "Epoch [4][15]\t Batch [180][550]\t Training Loss 2.0240\t Accuracy 0.2821\n",
      "Epoch [4][15]\t Batch [190][550]\t Training Loss 2.0247\t Accuracy 0.2823\n",
      "Epoch [4][15]\t Batch [200][550]\t Training Loss 2.0223\t Accuracy 0.2837\n",
      "Epoch [4][15]\t Batch [210][550]\t Training Loss 2.0228\t Accuracy 0.2832\n",
      "Epoch [4][15]\t Batch [220][550]\t Training Loss 2.0223\t Accuracy 0.2833\n",
      "Epoch [4][15]\t Batch [230][550]\t Training Loss 2.0230\t Accuracy 0.2838\n",
      "Epoch [4][15]\t Batch [240][550]\t Training Loss 2.0214\t Accuracy 0.2851\n",
      "Epoch [4][15]\t Batch [250][550]\t Training Loss 2.0201\t Accuracy 0.2860\n",
      "Epoch [4][15]\t Batch [260][550]\t Training Loss 2.0204\t Accuracy 0.2869\n",
      "Epoch [4][15]\t Batch [270][550]\t Training Loss 2.0201\t Accuracy 0.2870\n",
      "Epoch [4][15]\t Batch [280][550]\t Training Loss 2.0210\t Accuracy 0.2874\n",
      "Epoch [4][15]\t Batch [290][550]\t Training Loss 2.0203\t Accuracy 0.2878\n",
      "Epoch [4][15]\t Batch [300][550]\t Training Loss 2.0201\t Accuracy 0.2881\n",
      "Epoch [4][15]\t Batch [310][550]\t Training Loss 2.0194\t Accuracy 0.2884\n",
      "Epoch [4][15]\t Batch [320][550]\t Training Loss 2.0182\t Accuracy 0.2889\n",
      "Epoch [4][15]\t Batch [330][550]\t Training Loss 2.0173\t Accuracy 0.2890\n",
      "Epoch [4][15]\t Batch [340][550]\t Training Loss 2.0174\t Accuracy 0.2895\n",
      "Epoch [4][15]\t Batch [350][550]\t Training Loss 2.0183\t Accuracy 0.2891\n",
      "Epoch [4][15]\t Batch [360][550]\t Training Loss 2.0171\t Accuracy 0.2894\n",
      "Epoch [4][15]\t Batch [370][550]\t Training Loss 2.0164\t Accuracy 0.2898\n",
      "Epoch [4][15]\t Batch [380][550]\t Training Loss 2.0156\t Accuracy 0.2902\n",
      "Epoch [4][15]\t Batch [390][550]\t Training Loss 2.0140\t Accuracy 0.2909\n",
      "Epoch [4][15]\t Batch [400][550]\t Training Loss 2.0143\t Accuracy 0.2911\n",
      "Epoch [4][15]\t Batch [410][550]\t Training Loss 2.0136\t Accuracy 0.2910\n",
      "Epoch [4][15]\t Batch [420][550]\t Training Loss 2.0142\t Accuracy 0.2904\n",
      "Epoch [4][15]\t Batch [430][550]\t Training Loss 2.0141\t Accuracy 0.2907\n",
      "Epoch [4][15]\t Batch [440][550]\t Training Loss 2.0136\t Accuracy 0.2910\n",
      "Epoch [4][15]\t Batch [450][550]\t Training Loss 2.0126\t Accuracy 0.2912\n",
      "Epoch [4][15]\t Batch [460][550]\t Training Loss 2.0121\t Accuracy 0.2915\n",
      "Epoch [4][15]\t Batch [470][550]\t Training Loss 2.0108\t Accuracy 0.2917\n",
      "Epoch [4][15]\t Batch [480][550]\t Training Loss 2.0105\t Accuracy 0.2918\n",
      "Epoch [4][15]\t Batch [490][550]\t Training Loss 2.0101\t Accuracy 0.2920\n",
      "Epoch [4][15]\t Batch [500][550]\t Training Loss 2.0093\t Accuracy 0.2924\n",
      "Epoch [4][15]\t Batch [510][550]\t Training Loss 2.0095\t Accuracy 0.2923\n",
      "Epoch [4][15]\t Batch [520][550]\t Training Loss 2.0092\t Accuracy 0.2925\n",
      "Epoch [4][15]\t Batch [530][550]\t Training Loss 2.0081\t Accuracy 0.2935\n",
      "Epoch [4][15]\t Batch [540][550]\t Training Loss 2.0076\t Accuracy 0.2940\n",
      "\n",
      "Epoch [4]\t Average training loss 2.0072\t Average training accuracy 0.2941\n",
      "Epoch [4]\t Average validation loss 1.9872\t Average validation accuracy 0.3060\n",
      "\n",
      "Epoch [5][15]\t Batch [0][550]\t Training Loss 1.9366\t Accuracy 0.3400\n",
      "Epoch [5][15]\t Batch [10][550]\t Training Loss 1.9815\t Accuracy 0.3045\n",
      "Epoch [5][15]\t Batch [20][550]\t Training Loss 1.9814\t Accuracy 0.3090\n",
      "Epoch [5][15]\t Batch [30][550]\t Training Loss 1.9836\t Accuracy 0.3045\n",
      "Epoch [5][15]\t Batch [40][550]\t Training Loss 1.9855\t Accuracy 0.2978\n",
      "Epoch [5][15]\t Batch [50][550]\t Training Loss 1.9861\t Accuracy 0.3024\n",
      "Epoch [5][15]\t Batch [60][550]\t Training Loss 1.9827\t Accuracy 0.3008\n",
      "Epoch [5][15]\t Batch [70][550]\t Training Loss 1.9823\t Accuracy 0.3011\n",
      "Epoch [5][15]\t Batch [80][550]\t Training Loss 1.9801\t Accuracy 0.3026\n",
      "Epoch [5][15]\t Batch [90][550]\t Training Loss 1.9823\t Accuracy 0.3020\n",
      "Epoch [5][15]\t Batch [100][550]\t Training Loss 1.9789\t Accuracy 0.3033\n",
      "Epoch [5][15]\t Batch [110][550]\t Training Loss 1.9809\t Accuracy 0.3042\n",
      "Epoch [5][15]\t Batch [120][550]\t Training Loss 1.9818\t Accuracy 0.3036\n",
      "Epoch [5][15]\t Batch [130][550]\t Training Loss 1.9792\t Accuracy 0.3045\n",
      "Epoch [5][15]\t Batch [140][550]\t Training Loss 1.9802\t Accuracy 0.3045\n",
      "Epoch [5][15]\t Batch [150][550]\t Training Loss 1.9794\t Accuracy 0.3052\n",
      "Epoch [5][15]\t Batch [160][550]\t Training Loss 1.9795\t Accuracy 0.3058\n",
      "Epoch [5][15]\t Batch [170][550]\t Training Loss 1.9816\t Accuracy 0.3047\n",
      "Epoch [5][15]\t Batch [180][550]\t Training Loss 1.9827\t Accuracy 0.3038\n",
      "Epoch [5][15]\t Batch [190][550]\t Training Loss 1.9816\t Accuracy 0.3047\n",
      "Epoch [5][15]\t Batch [200][550]\t Training Loss 1.9796\t Accuracy 0.3052\n",
      "Epoch [5][15]\t Batch [210][550]\t Training Loss 1.9784\t Accuracy 0.3066\n",
      "Epoch [5][15]\t Batch [220][550]\t Training Loss 1.9795\t Accuracy 0.3062\n",
      "Epoch [5][15]\t Batch [230][550]\t Training Loss 1.9805\t Accuracy 0.3057\n",
      "Epoch [5][15]\t Batch [240][550]\t Training Loss 1.9785\t Accuracy 0.3066\n",
      "Epoch [5][15]\t Batch [250][550]\t Training Loss 1.9785\t Accuracy 0.3061\n",
      "Epoch [5][15]\t Batch [260][550]\t Training Loss 1.9776\t Accuracy 0.3069\n",
      "Epoch [5][15]\t Batch [270][550]\t Training Loss 1.9782\t Accuracy 0.3071\n",
      "Epoch [5][15]\t Batch [280][550]\t Training Loss 1.9788\t Accuracy 0.3062\n",
      "Epoch [5][15]\t Batch [290][550]\t Training Loss 1.9789\t Accuracy 0.3059\n",
      "Epoch [5][15]\t Batch [300][550]\t Training Loss 1.9785\t Accuracy 0.3070\n",
      "Epoch [5][15]\t Batch [310][550]\t Training Loss 1.9781\t Accuracy 0.3076\n",
      "Epoch [5][15]\t Batch [320][550]\t Training Loss 1.9773\t Accuracy 0.3078\n",
      "Epoch [5][15]\t Batch [330][550]\t Training Loss 1.9773\t Accuracy 0.3077\n",
      "Epoch [5][15]\t Batch [340][550]\t Training Loss 1.9769\t Accuracy 0.3076\n",
      "Epoch [5][15]\t Batch [350][550]\t Training Loss 1.9768\t Accuracy 0.3076\n",
      "Epoch [5][15]\t Batch [360][550]\t Training Loss 1.9763\t Accuracy 0.3073\n",
      "Epoch [5][15]\t Batch [370][550]\t Training Loss 1.9763\t Accuracy 0.3075\n",
      "Epoch [5][15]\t Batch [380][550]\t Training Loss 1.9761\t Accuracy 0.3080\n",
      "Epoch [5][15]\t Batch [390][550]\t Training Loss 1.9758\t Accuracy 0.3080\n",
      "Epoch [5][15]\t Batch [400][550]\t Training Loss 1.9743\t Accuracy 0.3091\n",
      "Epoch [5][15]\t Batch [410][550]\t Training Loss 1.9730\t Accuracy 0.3097\n",
      "Epoch [5][15]\t Batch [420][550]\t Training Loss 1.9722\t Accuracy 0.3097\n",
      "Epoch [5][15]\t Batch [430][550]\t Training Loss 1.9719\t Accuracy 0.3098\n",
      "Epoch [5][15]\t Batch [440][550]\t Training Loss 1.9718\t Accuracy 0.3100\n",
      "Epoch [5][15]\t Batch [450][550]\t Training Loss 1.9716\t Accuracy 0.3102\n",
      "Epoch [5][15]\t Batch [460][550]\t Training Loss 1.9710\t Accuracy 0.3106\n",
      "Epoch [5][15]\t Batch [470][550]\t Training Loss 1.9709\t Accuracy 0.3108\n",
      "Epoch [5][15]\t Batch [480][550]\t Training Loss 1.9697\t Accuracy 0.3115\n",
      "Epoch [5][15]\t Batch [490][550]\t Training Loss 1.9697\t Accuracy 0.3114\n",
      "Epoch [5][15]\t Batch [500][550]\t Training Loss 1.9698\t Accuracy 0.3115\n",
      "Epoch [5][15]\t Batch [510][550]\t Training Loss 1.9696\t Accuracy 0.3115\n",
      "Epoch [5][15]\t Batch [520][550]\t Training Loss 1.9693\t Accuracy 0.3117\n",
      "Epoch [5][15]\t Batch [530][550]\t Training Loss 1.9700\t Accuracy 0.3112\n",
      "Epoch [5][15]\t Batch [540][550]\t Training Loss 1.9703\t Accuracy 0.3112\n",
      "\n",
      "Epoch [5]\t Average training loss 1.9708\t Average training accuracy 0.3112\n",
      "Epoch [5]\t Average validation loss 1.9505\t Average validation accuracy 0.3130\n",
      "\n",
      "Epoch [6][15]\t Batch [0][550]\t Training Loss 1.9232\t Accuracy 0.2900\n",
      "Epoch [6][15]\t Batch [10][550]\t Training Loss 1.9474\t Accuracy 0.3045\n",
      "Epoch [6][15]\t Batch [20][550]\t Training Loss 1.9601\t Accuracy 0.3162\n",
      "Epoch [6][15]\t Batch [30][550]\t Training Loss 1.9701\t Accuracy 0.3129\n",
      "Epoch [6][15]\t Batch [40][550]\t Training Loss 1.9740\t Accuracy 0.3107\n",
      "Epoch [6][15]\t Batch [50][550]\t Training Loss 1.9731\t Accuracy 0.3120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6][15]\t Batch [60][550]\t Training Loss 1.9688\t Accuracy 0.3103\n",
      "Epoch [6][15]\t Batch [70][550]\t Training Loss 1.9664\t Accuracy 0.3120\n",
      "Epoch [6][15]\t Batch [80][550]\t Training Loss 1.9664\t Accuracy 0.3111\n",
      "Epoch [6][15]\t Batch [90][550]\t Training Loss 1.9683\t Accuracy 0.3092\n",
      "Epoch [6][15]\t Batch [100][550]\t Training Loss 1.9658\t Accuracy 0.3114\n",
      "Epoch [6][15]\t Batch [110][550]\t Training Loss 1.9639\t Accuracy 0.3131\n",
      "Epoch [6][15]\t Batch [120][550]\t Training Loss 1.9658\t Accuracy 0.3121\n",
      "Epoch [6][15]\t Batch [130][550]\t Training Loss 1.9633\t Accuracy 0.3140\n",
      "Epoch [6][15]\t Batch [140][550]\t Training Loss 1.9624\t Accuracy 0.3145\n",
      "Epoch [6][15]\t Batch [150][550]\t Training Loss 1.9616\t Accuracy 0.3156\n",
      "Epoch [6][15]\t Batch [160][550]\t Training Loss 1.9609\t Accuracy 0.3157\n",
      "Epoch [6][15]\t Batch [170][550]\t Training Loss 1.9603\t Accuracy 0.3148\n",
      "Epoch [6][15]\t Batch [180][550]\t Training Loss 1.9613\t Accuracy 0.3142\n",
      "Epoch [6][15]\t Batch [190][550]\t Training Loss 1.9628\t Accuracy 0.3132\n",
      "Epoch [6][15]\t Batch [200][550]\t Training Loss 1.9619\t Accuracy 0.3132\n",
      "Epoch [6][15]\t Batch [210][550]\t Training Loss 1.9624\t Accuracy 0.3136\n",
      "Epoch [6][15]\t Batch [220][550]\t Training Loss 1.9618\t Accuracy 0.3134\n",
      "Epoch [6][15]\t Batch [230][550]\t Training Loss 1.9596\t Accuracy 0.3147\n",
      "Epoch [6][15]\t Batch [240][550]\t Training Loss 1.9596\t Accuracy 0.3151\n",
      "Epoch [6][15]\t Batch [250][550]\t Training Loss 1.9606\t Accuracy 0.3153\n",
      "Epoch [6][15]\t Batch [260][550]\t Training Loss 1.9615\t Accuracy 0.3155\n",
      "Epoch [6][15]\t Batch [270][550]\t Training Loss 1.9610\t Accuracy 0.3151\n",
      "Epoch [6][15]\t Batch [280][550]\t Training Loss 1.9613\t Accuracy 0.3148\n",
      "Epoch [6][15]\t Batch [290][550]\t Training Loss 1.9620\t Accuracy 0.3141\n",
      "Epoch [6][15]\t Batch [300][550]\t Training Loss 1.9620\t Accuracy 0.3146\n",
      "Epoch [6][15]\t Batch [310][550]\t Training Loss 1.9604\t Accuracy 0.3154\n",
      "Epoch [6][15]\t Batch [320][550]\t Training Loss 1.9591\t Accuracy 0.3159\n",
      "Epoch [6][15]\t Batch [330][550]\t Training Loss 1.9585\t Accuracy 0.3155\n",
      "Epoch [6][15]\t Batch [340][550]\t Training Loss 1.9570\t Accuracy 0.3163\n",
      "Epoch [6][15]\t Batch [350][550]\t Training Loss 1.9572\t Accuracy 0.3161\n",
      "Epoch [6][15]\t Batch [360][550]\t Training Loss 1.9567\t Accuracy 0.3166\n",
      "Epoch [6][15]\t Batch [370][550]\t Training Loss 1.9559\t Accuracy 0.3168\n",
      "Epoch [6][15]\t Batch [380][550]\t Training Loss 1.9554\t Accuracy 0.3172\n",
      "Epoch [6][15]\t Batch [390][550]\t Training Loss 1.9551\t Accuracy 0.3174\n",
      "Epoch [6][15]\t Batch [400][550]\t Training Loss 1.9545\t Accuracy 0.3176\n",
      "Epoch [6][15]\t Batch [410][550]\t Training Loss 1.9546\t Accuracy 0.3177\n",
      "Epoch [6][15]\t Batch [420][550]\t Training Loss 1.9546\t Accuracy 0.3178\n",
      "Epoch [6][15]\t Batch [430][550]\t Training Loss 1.9535\t Accuracy 0.3181\n",
      "Epoch [6][15]\t Batch [440][550]\t Training Loss 1.9524\t Accuracy 0.3185\n",
      "Epoch [6][15]\t Batch [450][550]\t Training Loss 1.9525\t Accuracy 0.3185\n",
      "Epoch [6][15]\t Batch [460][550]\t Training Loss 1.9516\t Accuracy 0.3189\n",
      "Epoch [6][15]\t Batch [470][550]\t Training Loss 1.9512\t Accuracy 0.3194\n",
      "Epoch [6][15]\t Batch [480][550]\t Training Loss 1.9501\t Accuracy 0.3196\n",
      "Epoch [6][15]\t Batch [490][550]\t Training Loss 1.9495\t Accuracy 0.3198\n",
      "Epoch [6][15]\t Batch [500][550]\t Training Loss 1.9496\t Accuracy 0.3199\n",
      "Epoch [6][15]\t Batch [510][550]\t Training Loss 1.9495\t Accuracy 0.3198\n",
      "Epoch [6][15]\t Batch [520][550]\t Training Loss 1.9497\t Accuracy 0.3196\n",
      "Epoch [6][15]\t Batch [530][550]\t Training Loss 1.9492\t Accuracy 0.3200\n",
      "Epoch [6][15]\t Batch [540][550]\t Training Loss 1.9493\t Accuracy 0.3199\n",
      "\n",
      "Epoch [6]\t Average training loss 1.9495\t Average training accuracy 0.3197\n",
      "Epoch [6]\t Average validation loss 1.9455\t Average validation accuracy 0.3174\n",
      "\n",
      "Epoch [7][15]\t Batch [0][550]\t Training Loss 2.0205\t Accuracy 0.3100\n",
      "Epoch [7][15]\t Batch [10][550]\t Training Loss 1.9281\t Accuracy 0.3382\n",
      "Epoch [7][15]\t Batch [20][550]\t Training Loss 1.9372\t Accuracy 0.3310\n",
      "Epoch [7][15]\t Batch [30][550]\t Training Loss 1.9484\t Accuracy 0.3216\n",
      "Epoch [7][15]\t Batch [40][550]\t Training Loss 1.9471\t Accuracy 0.3210\n",
      "Epoch [7][15]\t Batch [50][550]\t Training Loss 1.9555\t Accuracy 0.3171\n",
      "Epoch [7][15]\t Batch [60][550]\t Training Loss 1.9544\t Accuracy 0.3175\n",
      "Epoch [7][15]\t Batch [70][550]\t Training Loss 1.9628\t Accuracy 0.3131\n",
      "Epoch [7][15]\t Batch [80][550]\t Training Loss 1.9580\t Accuracy 0.3146\n",
      "Epoch [7][15]\t Batch [90][550]\t Training Loss 1.9512\t Accuracy 0.3177\n",
      "Epoch [7][15]\t Batch [100][550]\t Training Loss 1.9503\t Accuracy 0.3180\n",
      "Epoch [7][15]\t Batch [110][550]\t Training Loss 1.9473\t Accuracy 0.3182\n",
      "Epoch [7][15]\t Batch [120][550]\t Training Loss 1.9434\t Accuracy 0.3207\n",
      "Epoch [7][15]\t Batch [130][550]\t Training Loss 1.9444\t Accuracy 0.3213\n",
      "Epoch [7][15]\t Batch [140][550]\t Training Loss 1.9426\t Accuracy 0.3218\n",
      "Epoch [7][15]\t Batch [150][550]\t Training Loss 1.9403\t Accuracy 0.3219\n",
      "Epoch [7][15]\t Batch [160][550]\t Training Loss 1.9423\t Accuracy 0.3222\n",
      "Epoch [7][15]\t Batch [170][550]\t Training Loss 1.9436\t Accuracy 0.3225\n",
      "Epoch [7][15]\t Batch [180][550]\t Training Loss 1.9435\t Accuracy 0.3223\n",
      "Epoch [7][15]\t Batch [190][550]\t Training Loss 1.9427\t Accuracy 0.3213\n",
      "Epoch [7][15]\t Batch [200][550]\t Training Loss 1.9424\t Accuracy 0.3214\n",
      "Epoch [7][15]\t Batch [210][550]\t Training Loss 1.9408\t Accuracy 0.3222\n",
      "Epoch [7][15]\t Batch [220][550]\t Training Loss 1.9417\t Accuracy 0.3219\n",
      "Epoch [7][15]\t Batch [230][550]\t Training Loss 1.9428\t Accuracy 0.3221\n",
      "Epoch [7][15]\t Batch [240][550]\t Training Loss 1.9416\t Accuracy 0.3225\n",
      "Epoch [7][15]\t Batch [250][550]\t Training Loss 1.9411\t Accuracy 0.3229\n",
      "Epoch [7][15]\t Batch [260][550]\t Training Loss 1.9394\t Accuracy 0.3233\n",
      "Epoch [7][15]\t Batch [270][550]\t Training Loss 1.9404\t Accuracy 0.3228\n",
      "Epoch [7][15]\t Batch [280][550]\t Training Loss 1.9393\t Accuracy 0.3231\n",
      "Epoch [7][15]\t Batch [290][550]\t Training Loss 1.9389\t Accuracy 0.3229\n",
      "Epoch [7][15]\t Batch [300][550]\t Training Loss 1.9387\t Accuracy 0.3229\n",
      "Epoch [7][15]\t Batch [310][550]\t Training Loss 1.9365\t Accuracy 0.3240\n",
      "Epoch [7][15]\t Batch [320][550]\t Training Loss 1.9362\t Accuracy 0.3241\n",
      "Epoch [7][15]\t Batch [330][550]\t Training Loss 1.9353\t Accuracy 0.3247\n",
      "Epoch [7][15]\t Batch [340][550]\t Training Loss 1.9360\t Accuracy 0.3248\n",
      "Epoch [7][15]\t Batch [350][550]\t Training Loss 1.9351\t Accuracy 0.3255\n",
      "Epoch [7][15]\t Batch [360][550]\t Training Loss 1.9344\t Accuracy 0.3259\n",
      "Epoch [7][15]\t Batch [370][550]\t Training Loss 1.9337\t Accuracy 0.3264\n",
      "Epoch [7][15]\t Batch [380][550]\t Training Loss 1.9340\t Accuracy 0.3262\n",
      "Epoch [7][15]\t Batch [390][550]\t Training Loss 1.9342\t Accuracy 0.3262\n",
      "Epoch [7][15]\t Batch [400][550]\t Training Loss 1.9337\t Accuracy 0.3264\n",
      "Epoch [7][15]\t Batch [410][550]\t Training Loss 1.9333\t Accuracy 0.3271\n",
      "Epoch [7][15]\t Batch [420][550]\t Training Loss 1.9330\t Accuracy 0.3274\n",
      "Epoch [7][15]\t Batch [430][550]\t Training Loss 1.9335\t Accuracy 0.3273\n",
      "Epoch [7][15]\t Batch [440][550]\t Training Loss 1.9330\t Accuracy 0.3275\n",
      "Epoch [7][15]\t Batch [450][550]\t Training Loss 1.9331\t Accuracy 0.3273\n",
      "Epoch [7][15]\t Batch [460][550]\t Training Loss 1.9330\t Accuracy 0.3279\n",
      "Epoch [7][15]\t Batch [470][550]\t Training Loss 1.9329\t Accuracy 0.3280\n",
      "Epoch [7][15]\t Batch [480][550]\t Training Loss 1.9328\t Accuracy 0.3285\n",
      "Epoch [7][15]\t Batch [490][550]\t Training Loss 1.9329\t Accuracy 0.3282\n",
      "Epoch [7][15]\t Batch [500][550]\t Training Loss 1.9331\t Accuracy 0.3283\n",
      "Epoch [7][15]\t Batch [510][550]\t Training Loss 1.9331\t Accuracy 0.3285\n",
      "Epoch [7][15]\t Batch [520][550]\t Training Loss 1.9341\t Accuracy 0.3279\n",
      "Epoch [7][15]\t Batch [530][550]\t Training Loss 1.9345\t Accuracy 0.3279\n",
      "Epoch [7][15]\t Batch [540][550]\t Training Loss 1.9344\t Accuracy 0.3283\n",
      "\n",
      "Epoch [7]\t Average training loss 1.9343\t Average training accuracy 0.3282\n",
      "Epoch [7]\t Average validation loss 1.9310\t Average validation accuracy 0.3296\n",
      "\n",
      "Epoch [8][15]\t Batch [0][550]\t Training Loss 1.9010\t Accuracy 0.3200\n",
      "Epoch [8][15]\t Batch [10][550]\t Training Loss 1.9145\t Accuracy 0.3300\n",
      "Epoch [8][15]\t Batch [20][550]\t Training Loss 1.9160\t Accuracy 0.3419\n",
      "Epoch [8][15]\t Batch [30][550]\t Training Loss 1.9006\t Accuracy 0.3474\n",
      "Epoch [8][15]\t Batch [40][550]\t Training Loss 1.9005\t Accuracy 0.3468\n",
      "Epoch [8][15]\t Batch [50][550]\t Training Loss 1.9123\t Accuracy 0.3400\n",
      "Epoch [8][15]\t Batch [60][550]\t Training Loss 1.9183\t Accuracy 0.3408\n",
      "Epoch [8][15]\t Batch [70][550]\t Training Loss 1.9226\t Accuracy 0.3363\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8][15]\t Batch [80][550]\t Training Loss 1.9267\t Accuracy 0.3343\n",
      "Epoch [8][15]\t Batch [90][550]\t Training Loss 1.9226\t Accuracy 0.3355\n",
      "Epoch [8][15]\t Batch [100][550]\t Training Loss 1.9282\t Accuracy 0.3319\n",
      "Epoch [8][15]\t Batch [110][550]\t Training Loss 1.9298\t Accuracy 0.3306\n",
      "Epoch [8][15]\t Batch [120][550]\t Training Loss 1.9324\t Accuracy 0.3300\n",
      "Epoch [8][15]\t Batch [130][550]\t Training Loss 1.9339\t Accuracy 0.3294\n",
      "Epoch [8][15]\t Batch [140][550]\t Training Loss 1.9320\t Accuracy 0.3299\n",
      "Epoch [8][15]\t Batch [150][550]\t Training Loss 1.9314\t Accuracy 0.3302\n",
      "Epoch [8][15]\t Batch [160][550]\t Training Loss 1.9272\t Accuracy 0.3322\n",
      "Epoch [8][15]\t Batch [170][550]\t Training Loss 1.9264\t Accuracy 0.3318\n",
      "Epoch [8][15]\t Batch [180][550]\t Training Loss 1.9271\t Accuracy 0.3314\n",
      "Epoch [8][15]\t Batch [190][550]\t Training Loss 1.9299\t Accuracy 0.3297\n",
      "Epoch [8][15]\t Batch [200][550]\t Training Loss 1.9300\t Accuracy 0.3299\n",
      "Epoch [8][15]\t Batch [210][550]\t Training Loss 1.9299\t Accuracy 0.3303\n",
      "Epoch [8][15]\t Batch [220][550]\t Training Loss 1.9314\t Accuracy 0.3302\n",
      "Epoch [8][15]\t Batch [230][550]\t Training Loss 1.9290\t Accuracy 0.3309\n",
      "Epoch [8][15]\t Batch [240][550]\t Training Loss 1.9284\t Accuracy 0.3309\n",
      "Epoch [8][15]\t Batch [250][550]\t Training Loss 1.9280\t Accuracy 0.3313\n",
      "Epoch [8][15]\t Batch [260][550]\t Training Loss 1.9282\t Accuracy 0.3313\n",
      "Epoch [8][15]\t Batch [270][550]\t Training Loss 1.9280\t Accuracy 0.3311\n",
      "Epoch [8][15]\t Batch [280][550]\t Training Loss 1.9285\t Accuracy 0.3311\n",
      "Epoch [8][15]\t Batch [290][550]\t Training Loss 1.9277\t Accuracy 0.3314\n",
      "Epoch [8][15]\t Batch [300][550]\t Training Loss 1.9273\t Accuracy 0.3309\n",
      "Epoch [8][15]\t Batch [310][550]\t Training Loss 1.9266\t Accuracy 0.3305\n",
      "Epoch [8][15]\t Batch [320][550]\t Training Loss 1.9255\t Accuracy 0.3310\n",
      "Epoch [8][15]\t Batch [330][550]\t Training Loss 1.9268\t Accuracy 0.3310\n",
      "Epoch [8][15]\t Batch [340][550]\t Training Loss 1.9263\t Accuracy 0.3314\n",
      "Epoch [8][15]\t Batch [350][550]\t Training Loss 1.9256\t Accuracy 0.3317\n",
      "Epoch [8][15]\t Batch [360][550]\t Training Loss 1.9262\t Accuracy 0.3312\n",
      "Epoch [8][15]\t Batch [370][550]\t Training Loss 1.9258\t Accuracy 0.3318\n",
      "Epoch [8][15]\t Batch [380][550]\t Training Loss 1.9256\t Accuracy 0.3319\n",
      "Epoch [8][15]\t Batch [390][550]\t Training Loss 1.9261\t Accuracy 0.3319\n",
      "Epoch [8][15]\t Batch [400][550]\t Training Loss 1.9268\t Accuracy 0.3318\n",
      "Epoch [8][15]\t Batch [410][550]\t Training Loss 1.9270\t Accuracy 0.3320\n",
      "Epoch [8][15]\t Batch [420][550]\t Training Loss 1.9270\t Accuracy 0.3319\n",
      "Epoch [8][15]\t Batch [430][550]\t Training Loss 1.9275\t Accuracy 0.3317\n",
      "Epoch [8][15]\t Batch [440][550]\t Training Loss 1.9272\t Accuracy 0.3323\n",
      "Epoch [8][15]\t Batch [450][550]\t Training Loss 1.9276\t Accuracy 0.3319\n",
      "Epoch [8][15]\t Batch [460][550]\t Training Loss 1.9275\t Accuracy 0.3321\n",
      "Epoch [8][15]\t Batch [470][550]\t Training Loss 1.9263\t Accuracy 0.3328\n",
      "Epoch [8][15]\t Batch [480][550]\t Training Loss 1.9258\t Accuracy 0.3335\n",
      "Epoch [8][15]\t Batch [490][550]\t Training Loss 1.9264\t Accuracy 0.3332\n",
      "Epoch [8][15]\t Batch [500][550]\t Training Loss 1.9261\t Accuracy 0.3331\n",
      "Epoch [8][15]\t Batch [510][550]\t Training Loss 1.9257\t Accuracy 0.3334\n",
      "Epoch [8][15]\t Batch [520][550]\t Training Loss 1.9264\t Accuracy 0.3330\n",
      "Epoch [8][15]\t Batch [530][550]\t Training Loss 1.9266\t Accuracy 0.3328\n",
      "Epoch [8][15]\t Batch [540][550]\t Training Loss 1.9258\t Accuracy 0.3331\n",
      "\n",
      "Epoch [8]\t Average training loss 1.9263\t Average training accuracy 0.3332\n",
      "Epoch [8]\t Average validation loss 1.9169\t Average validation accuracy 0.3412\n",
      "\n",
      "Epoch [9][15]\t Batch [0][550]\t Training Loss 1.9608\t Accuracy 0.3600\n",
      "Epoch [9][15]\t Batch [10][550]\t Training Loss 1.9379\t Accuracy 0.3273\n",
      "Epoch [9][15]\t Batch [20][550]\t Training Loss 1.9165\t Accuracy 0.3305\n",
      "Epoch [9][15]\t Batch [30][550]\t Training Loss 1.9181\t Accuracy 0.3268\n",
      "Epoch [9][15]\t Batch [40][550]\t Training Loss 1.9041\t Accuracy 0.3337\n",
      "Epoch [9][15]\t Batch [50][550]\t Training Loss 1.9099\t Accuracy 0.3347\n",
      "Epoch [9][15]\t Batch [60][550]\t Training Loss 1.9121\t Accuracy 0.3379\n",
      "Epoch [9][15]\t Batch [70][550]\t Training Loss 1.9086\t Accuracy 0.3393\n",
      "Epoch [9][15]\t Batch [80][550]\t Training Loss 1.9062\t Accuracy 0.3406\n",
      "Epoch [9][15]\t Batch [90][550]\t Training Loss 1.9083\t Accuracy 0.3398\n",
      "Epoch [9][15]\t Batch [100][550]\t Training Loss 1.9152\t Accuracy 0.3368\n",
      "Epoch [9][15]\t Batch [110][550]\t Training Loss 1.9128\t Accuracy 0.3387\n",
      "Epoch [9][15]\t Batch [120][550]\t Training Loss 1.9128\t Accuracy 0.3379\n",
      "Epoch [9][15]\t Batch [130][550]\t Training Loss 1.9156\t Accuracy 0.3355\n",
      "Epoch [9][15]\t Batch [140][550]\t Training Loss 1.9164\t Accuracy 0.3356\n",
      "Epoch [9][15]\t Batch [150][550]\t Training Loss 1.9140\t Accuracy 0.3358\n",
      "Epoch [9][15]\t Batch [160][550]\t Training Loss 1.9133\t Accuracy 0.3361\n",
      "Epoch [9][15]\t Batch [170][550]\t Training Loss 1.9096\t Accuracy 0.3373\n",
      "Epoch [9][15]\t Batch [180][550]\t Training Loss 1.9086\t Accuracy 0.3381\n",
      "Epoch [9][15]\t Batch [190][550]\t Training Loss 1.9079\t Accuracy 0.3376\n",
      "Epoch [9][15]\t Batch [200][550]\t Training Loss 1.9090\t Accuracy 0.3377\n",
      "Epoch [9][15]\t Batch [210][550]\t Training Loss 1.9128\t Accuracy 0.3365\n",
      "Epoch [9][15]\t Batch [220][550]\t Training Loss 1.9125\t Accuracy 0.3361\n",
      "Epoch [9][15]\t Batch [230][550]\t Training Loss 1.9127\t Accuracy 0.3364\n",
      "Epoch [9][15]\t Batch [240][550]\t Training Loss 1.9125\t Accuracy 0.3365\n",
      "Epoch [9][15]\t Batch [250][550]\t Training Loss 1.9140\t Accuracy 0.3363\n",
      "Epoch [9][15]\t Batch [260][550]\t Training Loss 1.9150\t Accuracy 0.3357\n",
      "Epoch [9][15]\t Batch [270][550]\t Training Loss 1.9154\t Accuracy 0.3361\n",
      "Epoch [9][15]\t Batch [280][550]\t Training Loss 1.9176\t Accuracy 0.3351\n",
      "Epoch [9][15]\t Batch [290][550]\t Training Loss 1.9161\t Accuracy 0.3355\n",
      "Epoch [9][15]\t Batch [300][550]\t Training Loss 1.9178\t Accuracy 0.3353\n",
      "Epoch [9][15]\t Batch [310][550]\t Training Loss 1.9178\t Accuracy 0.3355\n",
      "Epoch [9][15]\t Batch [320][550]\t Training Loss 1.9179\t Accuracy 0.3358\n",
      "Epoch [9][15]\t Batch [330][550]\t Training Loss 1.9175\t Accuracy 0.3363\n",
      "Epoch [9][15]\t Batch [340][550]\t Training Loss 1.9150\t Accuracy 0.3373\n",
      "Epoch [9][15]\t Batch [350][550]\t Training Loss 1.9156\t Accuracy 0.3376\n",
      "Epoch [9][15]\t Batch [360][550]\t Training Loss 1.9157\t Accuracy 0.3378\n",
      "Epoch [9][15]\t Batch [370][550]\t Training Loss 1.9155\t Accuracy 0.3377\n",
      "Epoch [9][15]\t Batch [380][550]\t Training Loss 1.9135\t Accuracy 0.3385\n",
      "Epoch [9][15]\t Batch [390][550]\t Training Loss 1.9135\t Accuracy 0.3384\n",
      "Epoch [9][15]\t Batch [400][550]\t Training Loss 1.9128\t Accuracy 0.3388\n",
      "Epoch [9][15]\t Batch [410][550]\t Training Loss 1.9121\t Accuracy 0.3395\n",
      "Epoch [9][15]\t Batch [420][550]\t Training Loss 1.9120\t Accuracy 0.3393\n",
      "Epoch [9][15]\t Batch [430][550]\t Training Loss 1.9128\t Accuracy 0.3389\n",
      "Epoch [9][15]\t Batch [440][550]\t Training Loss 1.9122\t Accuracy 0.3389\n",
      "Epoch [9][15]\t Batch [450][550]\t Training Loss 1.9125\t Accuracy 0.3391\n",
      "Epoch [9][15]\t Batch [460][550]\t Training Loss 1.9112\t Accuracy 0.3397\n",
      "Epoch [9][15]\t Batch [470][550]\t Training Loss 1.9115\t Accuracy 0.3396\n",
      "Epoch [9][15]\t Batch [480][550]\t Training Loss 1.9114\t Accuracy 0.3396\n",
      "Epoch [9][15]\t Batch [490][550]\t Training Loss 1.9104\t Accuracy 0.3398\n",
      "Epoch [9][15]\t Batch [500][550]\t Training Loss 1.9102\t Accuracy 0.3398\n",
      "Epoch [9][15]\t Batch [510][550]\t Training Loss 1.9106\t Accuracy 0.3396\n",
      "Epoch [9][15]\t Batch [520][550]\t Training Loss 1.9101\t Accuracy 0.3400\n",
      "Epoch [9][15]\t Batch [530][550]\t Training Loss 1.9103\t Accuracy 0.3396\n",
      "Epoch [9][15]\t Batch [540][550]\t Training Loss 1.9106\t Accuracy 0.3398\n",
      "\n",
      "Epoch [9]\t Average training loss 1.9104\t Average training accuracy 0.3399\n",
      "Epoch [9]\t Average validation loss 1.9082\t Average validation accuracy 0.3350\n",
      "\n",
      "Epoch [10][15]\t Batch [0][550]\t Training Loss 1.9636\t Accuracy 0.3900\n",
      "Epoch [10][15]\t Batch [10][550]\t Training Loss 1.9294\t Accuracy 0.3264\n",
      "Epoch [10][15]\t Batch [20][550]\t Training Loss 1.9105\t Accuracy 0.3267\n",
      "Epoch [10][15]\t Batch [30][550]\t Training Loss 1.9103\t Accuracy 0.3290\n",
      "Epoch [10][15]\t Batch [40][550]\t Training Loss 1.9106\t Accuracy 0.3302\n",
      "Epoch [10][15]\t Batch [50][550]\t Training Loss 1.9180\t Accuracy 0.3306\n",
      "Epoch [10][15]\t Batch [60][550]\t Training Loss 1.9150\t Accuracy 0.3338\n",
      "Epoch [10][15]\t Batch [70][550]\t Training Loss 1.9167\t Accuracy 0.3351\n",
      "Epoch [10][15]\t Batch [80][550]\t Training Loss 1.9164\t Accuracy 0.3356\n",
      "Epoch [10][15]\t Batch [90][550]\t Training Loss 1.9152\t Accuracy 0.3363\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10][15]\t Batch [100][550]\t Training Loss 1.9123\t Accuracy 0.3365\n",
      "Epoch [10][15]\t Batch [110][550]\t Training Loss 1.9109\t Accuracy 0.3361\n",
      "Epoch [10][15]\t Batch [120][550]\t Training Loss 1.9075\t Accuracy 0.3362\n",
      "Epoch [10][15]\t Batch [130][550]\t Training Loss 1.9071\t Accuracy 0.3356\n",
      "Epoch [10][15]\t Batch [140][550]\t Training Loss 1.9081\t Accuracy 0.3352\n",
      "Epoch [10][15]\t Batch [150][550]\t Training Loss 1.9088\t Accuracy 0.3358\n",
      "Epoch [10][15]\t Batch [160][550]\t Training Loss 1.9106\t Accuracy 0.3350\n",
      "Epoch [10][15]\t Batch [170][550]\t Training Loss 1.9127\t Accuracy 0.3347\n",
      "Epoch [10][15]\t Batch [180][550]\t Training Loss 1.9133\t Accuracy 0.3346\n",
      "Epoch [10][15]\t Batch [190][550]\t Training Loss 1.9115\t Accuracy 0.3350\n",
      "Epoch [10][15]\t Batch [200][550]\t Training Loss 1.9112\t Accuracy 0.3354\n",
      "Epoch [10][15]\t Batch [210][550]\t Training Loss 1.9101\t Accuracy 0.3362\n",
      "Epoch [10][15]\t Batch [220][550]\t Training Loss 1.9083\t Accuracy 0.3369\n",
      "Epoch [10][15]\t Batch [230][550]\t Training Loss 1.9056\t Accuracy 0.3377\n",
      "Epoch [10][15]\t Batch [240][550]\t Training Loss 1.9062\t Accuracy 0.3376\n",
      "Epoch [10][15]\t Batch [250][550]\t Training Loss 1.9062\t Accuracy 0.3375\n",
      "Epoch [10][15]\t Batch [260][550]\t Training Loss 1.9058\t Accuracy 0.3373\n",
      "Epoch [10][15]\t Batch [270][550]\t Training Loss 1.9064\t Accuracy 0.3377\n",
      "Epoch [10][15]\t Batch [280][550]\t Training Loss 1.9067\t Accuracy 0.3377\n",
      "Epoch [10][15]\t Batch [290][550]\t Training Loss 1.9063\t Accuracy 0.3384\n",
      "Epoch [10][15]\t Batch [300][550]\t Training Loss 1.9074\t Accuracy 0.3379\n",
      "Epoch [10][15]\t Batch [310][550]\t Training Loss 1.9060\t Accuracy 0.3382\n",
      "Epoch [10][15]\t Batch [320][550]\t Training Loss 1.9054\t Accuracy 0.3381\n",
      "Epoch [10][15]\t Batch [330][550]\t Training Loss 1.9058\t Accuracy 0.3379\n",
      "Epoch [10][15]\t Batch [340][550]\t Training Loss 1.9065\t Accuracy 0.3384\n",
      "Epoch [10][15]\t Batch [350][550]\t Training Loss 1.9061\t Accuracy 0.3389\n",
      "Epoch [10][15]\t Batch [360][550]\t Training Loss 1.9076\t Accuracy 0.3380\n",
      "Epoch [10][15]\t Batch [370][550]\t Training Loss 1.9072\t Accuracy 0.3384\n",
      "Epoch [10][15]\t Batch [380][550]\t Training Loss 1.9075\t Accuracy 0.3384\n",
      "Epoch [10][15]\t Batch [390][550]\t Training Loss 1.9073\t Accuracy 0.3383\n",
      "Epoch [10][15]\t Batch [400][550]\t Training Loss 1.9065\t Accuracy 0.3384\n",
      "Epoch [10][15]\t Batch [410][550]\t Training Loss 1.9069\t Accuracy 0.3387\n",
      "Epoch [10][15]\t Batch [420][550]\t Training Loss 1.9078\t Accuracy 0.3383\n",
      "Epoch [10][15]\t Batch [430][550]\t Training Loss 1.9081\t Accuracy 0.3382\n",
      "Epoch [10][15]\t Batch [440][550]\t Training Loss 1.9071\t Accuracy 0.3392\n",
      "Epoch [10][15]\t Batch [450][550]\t Training Loss 1.9071\t Accuracy 0.3396\n",
      "Epoch [10][15]\t Batch [460][550]\t Training Loss 1.9073\t Accuracy 0.3396\n",
      "Epoch [10][15]\t Batch [470][550]\t Training Loss 1.9069\t Accuracy 0.3398\n",
      "Epoch [10][15]\t Batch [480][550]\t Training Loss 1.9074\t Accuracy 0.3400\n",
      "Epoch [10][15]\t Batch [490][550]\t Training Loss 1.9068\t Accuracy 0.3401\n",
      "Epoch [10][15]\t Batch [500][550]\t Training Loss 1.9065\t Accuracy 0.3405\n",
      "Epoch [10][15]\t Batch [510][550]\t Training Loss 1.9065\t Accuracy 0.3405\n",
      "Epoch [10][15]\t Batch [520][550]\t Training Loss 1.9056\t Accuracy 0.3409\n",
      "Epoch [10][15]\t Batch [530][550]\t Training Loss 1.9052\t Accuracy 0.3411\n",
      "Epoch [10][15]\t Batch [540][550]\t Training Loss 1.9053\t Accuracy 0.3412\n",
      "\n",
      "Epoch [10]\t Average training loss 1.9049\t Average training accuracy 0.3412\n",
      "Epoch [10]\t Average validation loss 1.9018\t Average validation accuracy 0.3474\n",
      "\n",
      "Epoch [11][15]\t Batch [0][550]\t Training Loss 1.7629\t Accuracy 0.3900\n",
      "Epoch [11][15]\t Batch [10][550]\t Training Loss 1.8607\t Accuracy 0.3482\n",
      "Epoch [11][15]\t Batch [20][550]\t Training Loss 1.9011\t Accuracy 0.3352\n",
      "Epoch [11][15]\t Batch [30][550]\t Training Loss 1.9083\t Accuracy 0.3352\n",
      "Epoch [11][15]\t Batch [40][550]\t Training Loss 1.9179\t Accuracy 0.3356\n",
      "Epoch [11][15]\t Batch [50][550]\t Training Loss 1.9141\t Accuracy 0.3384\n",
      "Epoch [11][15]\t Batch [60][550]\t Training Loss 1.9101\t Accuracy 0.3407\n",
      "Epoch [11][15]\t Batch [70][550]\t Training Loss 1.9150\t Accuracy 0.3382\n",
      "Epoch [11][15]\t Batch [80][550]\t Training Loss 1.9160\t Accuracy 0.3381\n",
      "Epoch [11][15]\t Batch [90][550]\t Training Loss 1.9173\t Accuracy 0.3356\n",
      "Epoch [11][15]\t Batch [100][550]\t Training Loss 1.9165\t Accuracy 0.3364\n",
      "Epoch [11][15]\t Batch [110][550]\t Training Loss 1.9209\t Accuracy 0.3361\n",
      "Epoch [11][15]\t Batch [120][550]\t Training Loss 1.9176\t Accuracy 0.3374\n",
      "Epoch [11][15]\t Batch [130][550]\t Training Loss 1.9182\t Accuracy 0.3372\n",
      "Epoch [11][15]\t Batch [140][550]\t Training Loss 1.9154\t Accuracy 0.3387\n",
      "Epoch [11][15]\t Batch [150][550]\t Training Loss 1.9147\t Accuracy 0.3390\n",
      "Epoch [11][15]\t Batch [160][550]\t Training Loss 1.9138\t Accuracy 0.3391\n",
      "Epoch [11][15]\t Batch [170][550]\t Training Loss 1.9163\t Accuracy 0.3370\n",
      "Epoch [11][15]\t Batch [180][550]\t Training Loss 1.9149\t Accuracy 0.3380\n",
      "Epoch [11][15]\t Batch [190][550]\t Training Loss 1.9135\t Accuracy 0.3385\n",
      "Epoch [11][15]\t Batch [200][550]\t Training Loss 1.9149\t Accuracy 0.3392\n",
      "Epoch [11][15]\t Batch [210][550]\t Training Loss 1.9152\t Accuracy 0.3385\n",
      "Epoch [11][15]\t Batch [220][550]\t Training Loss 1.9122\t Accuracy 0.3396\n",
      "Epoch [11][15]\t Batch [230][550]\t Training Loss 1.9118\t Accuracy 0.3398\n",
      "Epoch [11][15]\t Batch [240][550]\t Training Loss 1.9117\t Accuracy 0.3402\n",
      "Epoch [11][15]\t Batch [250][550]\t Training Loss 1.9098\t Accuracy 0.3404\n",
      "Epoch [11][15]\t Batch [260][550]\t Training Loss 1.9099\t Accuracy 0.3399\n",
      "Epoch [11][15]\t Batch [270][550]\t Training Loss 1.9111\t Accuracy 0.3391\n",
      "Epoch [11][15]\t Batch [280][550]\t Training Loss 1.9109\t Accuracy 0.3391\n",
      "Epoch [11][15]\t Batch [290][550]\t Training Loss 1.9111\t Accuracy 0.3392\n",
      "Epoch [11][15]\t Batch [300][550]\t Training Loss 1.9108\t Accuracy 0.3399\n",
      "Epoch [11][15]\t Batch [310][550]\t Training Loss 1.9092\t Accuracy 0.3399\n",
      "Epoch [11][15]\t Batch [320][550]\t Training Loss 1.9086\t Accuracy 0.3401\n",
      "Epoch [11][15]\t Batch [330][550]\t Training Loss 1.9088\t Accuracy 0.3397\n",
      "Epoch [11][15]\t Batch [340][550]\t Training Loss 1.9081\t Accuracy 0.3399\n",
      "Epoch [11][15]\t Batch [350][550]\t Training Loss 1.9068\t Accuracy 0.3406\n",
      "Epoch [11][15]\t Batch [360][550]\t Training Loss 1.9062\t Accuracy 0.3411\n",
      "Epoch [11][15]\t Batch [370][550]\t Training Loss 1.9073\t Accuracy 0.3408\n",
      "Epoch [11][15]\t Batch [380][550]\t Training Loss 1.9089\t Accuracy 0.3407\n",
      "Epoch [11][15]\t Batch [390][550]\t Training Loss 1.9097\t Accuracy 0.3408\n",
      "Epoch [11][15]\t Batch [400][550]\t Training Loss 1.9096\t Accuracy 0.3409\n",
      "Epoch [11][15]\t Batch [410][550]\t Training Loss 1.9100\t Accuracy 0.3409\n",
      "Epoch [11][15]\t Batch [420][550]\t Training Loss 1.9092\t Accuracy 0.3408\n",
      "Epoch [11][15]\t Batch [430][550]\t Training Loss 1.9082\t Accuracy 0.3409\n",
      "Epoch [11][15]\t Batch [440][550]\t Training Loss 1.9079\t Accuracy 0.3410\n",
      "Epoch [11][15]\t Batch [450][550]\t Training Loss 1.9082\t Accuracy 0.3406\n",
      "Epoch [11][15]\t Batch [460][550]\t Training Loss 1.9068\t Accuracy 0.3412\n",
      "Epoch [11][15]\t Batch [470][550]\t Training Loss 1.9069\t Accuracy 0.3411\n",
      "Epoch [11][15]\t Batch [480][550]\t Training Loss 1.9063\t Accuracy 0.3418\n",
      "Epoch [11][15]\t Batch [490][550]\t Training Loss 1.9063\t Accuracy 0.3416\n",
      "Epoch [11][15]\t Batch [500][550]\t Training Loss 1.9054\t Accuracy 0.3421\n",
      "Epoch [11][15]\t Batch [510][550]\t Training Loss 1.9052\t Accuracy 0.3418\n",
      "Epoch [11][15]\t Batch [520][550]\t Training Loss 1.9058\t Accuracy 0.3420\n",
      "Epoch [11][15]\t Batch [530][550]\t Training Loss 1.9064\t Accuracy 0.3414\n",
      "Epoch [11][15]\t Batch [540][550]\t Training Loss 1.9064\t Accuracy 0.3417\n",
      "\n",
      "Epoch [11]\t Average training loss 1.9066\t Average training accuracy 0.3415\n",
      "Epoch [11]\t Average validation loss 1.9062\t Average validation accuracy 0.3366\n",
      "\n",
      "Epoch [12][15]\t Batch [0][550]\t Training Loss 2.0011\t Accuracy 0.3200\n",
      "Epoch [12][15]\t Batch [10][550]\t Training Loss 1.9119\t Accuracy 0.3545\n",
      "Epoch [12][15]\t Batch [20][550]\t Training Loss 1.9314\t Accuracy 0.3348\n",
      "Epoch [12][15]\t Batch [30][550]\t Training Loss 1.9196\t Accuracy 0.3394\n",
      "Epoch [12][15]\t Batch [40][550]\t Training Loss 1.9052\t Accuracy 0.3410\n",
      "Epoch [12][15]\t Batch [50][550]\t Training Loss 1.8941\t Accuracy 0.3439\n",
      "Epoch [12][15]\t Batch [60][550]\t Training Loss 1.8898\t Accuracy 0.3431\n",
      "Epoch [12][15]\t Batch [70][550]\t Training Loss 1.8894\t Accuracy 0.3437\n",
      "Epoch [12][15]\t Batch [80][550]\t Training Loss 1.8895\t Accuracy 0.3414\n",
      "Epoch [12][15]\t Batch [90][550]\t Training Loss 1.8891\t Accuracy 0.3413\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12][15]\t Batch [100][550]\t Training Loss 1.8943\t Accuracy 0.3387\n",
      "Epoch [12][15]\t Batch [110][550]\t Training Loss 1.8945\t Accuracy 0.3389\n",
      "Epoch [12][15]\t Batch [120][550]\t Training Loss 1.8942\t Accuracy 0.3391\n",
      "Epoch [12][15]\t Batch [130][550]\t Training Loss 1.8964\t Accuracy 0.3378\n",
      "Epoch [12][15]\t Batch [140][550]\t Training Loss 1.8953\t Accuracy 0.3381\n",
      "Epoch [12][15]\t Batch [150][550]\t Training Loss 1.8956\t Accuracy 0.3383\n",
      "Epoch [12][15]\t Batch [160][550]\t Training Loss 1.8957\t Accuracy 0.3388\n",
      "Epoch [12][15]\t Batch [170][550]\t Training Loss 1.8970\t Accuracy 0.3390\n",
      "Epoch [12][15]\t Batch [180][550]\t Training Loss 1.8963\t Accuracy 0.3400\n",
      "Epoch [12][15]\t Batch [190][550]\t Training Loss 1.8957\t Accuracy 0.3398\n",
      "Epoch [12][15]\t Batch [200][550]\t Training Loss 1.8960\t Accuracy 0.3401\n",
      "Epoch [12][15]\t Batch [210][550]\t Training Loss 1.8939\t Accuracy 0.3414\n",
      "Epoch [12][15]\t Batch [220][550]\t Training Loss 1.8944\t Accuracy 0.3413\n",
      "Epoch [12][15]\t Batch [230][550]\t Training Loss 1.8928\t Accuracy 0.3426\n",
      "Epoch [12][15]\t Batch [240][550]\t Training Loss 1.8953\t Accuracy 0.3421\n",
      "Epoch [12][15]\t Batch [250][550]\t Training Loss 1.8960\t Accuracy 0.3424\n",
      "Epoch [12][15]\t Batch [260][550]\t Training Loss 1.8951\t Accuracy 0.3433\n",
      "Epoch [12][15]\t Batch [270][550]\t Training Loss 1.8954\t Accuracy 0.3431\n",
      "Epoch [12][15]\t Batch [280][550]\t Training Loss 1.8946\t Accuracy 0.3432\n",
      "Epoch [12][15]\t Batch [290][550]\t Training Loss 1.8956\t Accuracy 0.3424\n",
      "Epoch [12][15]\t Batch [300][550]\t Training Loss 1.8964\t Accuracy 0.3425\n",
      "Epoch [12][15]\t Batch [310][550]\t Training Loss 1.8962\t Accuracy 0.3423\n",
      "Epoch [12][15]\t Batch [320][550]\t Training Loss 1.8959\t Accuracy 0.3421\n",
      "Epoch [12][15]\t Batch [330][550]\t Training Loss 1.8960\t Accuracy 0.3423\n",
      "Epoch [12][15]\t Batch [340][550]\t Training Loss 1.8963\t Accuracy 0.3418\n",
      "Epoch [12][15]\t Batch [350][550]\t Training Loss 1.8976\t Accuracy 0.3417\n",
      "Epoch [12][15]\t Batch [360][550]\t Training Loss 1.8979\t Accuracy 0.3415\n",
      "Epoch [12][15]\t Batch [370][550]\t Training Loss 1.8971\t Accuracy 0.3415\n",
      "Epoch [12][15]\t Batch [380][550]\t Training Loss 1.8968\t Accuracy 0.3415\n",
      "Epoch [12][15]\t Batch [390][550]\t Training Loss 1.8958\t Accuracy 0.3414\n",
      "Epoch [12][15]\t Batch [400][550]\t Training Loss 1.8967\t Accuracy 0.3407\n",
      "Epoch [12][15]\t Batch [410][550]\t Training Loss 1.8978\t Accuracy 0.3404\n",
      "Epoch [12][15]\t Batch [420][550]\t Training Loss 1.8988\t Accuracy 0.3401\n",
      "Epoch [12][15]\t Batch [430][550]\t Training Loss 1.8994\t Accuracy 0.3402\n",
      "Epoch [12][15]\t Batch [440][550]\t Training Loss 1.8999\t Accuracy 0.3405\n",
      "Epoch [12][15]\t Batch [450][550]\t Training Loss 1.8998\t Accuracy 0.3400\n",
      "Epoch [12][15]\t Batch [460][550]\t Training Loss 1.8994\t Accuracy 0.3404\n",
      "Epoch [12][15]\t Batch [470][550]\t Training Loss 1.8992\t Accuracy 0.3403\n",
      "Epoch [12][15]\t Batch [480][550]\t Training Loss 1.8994\t Accuracy 0.3404\n",
      "Epoch [12][15]\t Batch [490][550]\t Training Loss 1.8989\t Accuracy 0.3404\n",
      "Epoch [12][15]\t Batch [500][550]\t Training Loss 1.8991\t Accuracy 0.3407\n",
      "Epoch [12][15]\t Batch [510][550]\t Training Loss 1.8998\t Accuracy 0.3405\n",
      "Epoch [12][15]\t Batch [520][550]\t Training Loss 1.8994\t Accuracy 0.3407\n",
      "Epoch [12][15]\t Batch [530][550]\t Training Loss 1.8996\t Accuracy 0.3410\n",
      "Epoch [12][15]\t Batch [540][550]\t Training Loss 1.8989\t Accuracy 0.3411\n",
      "\n",
      "Epoch [12]\t Average training loss 1.8985\t Average training accuracy 0.3410\n",
      "Epoch [12]\t Average validation loss 1.8995\t Average validation accuracy 0.3422\n",
      "\n",
      "Epoch [13][15]\t Batch [0][550]\t Training Loss 1.9330\t Accuracy 0.2800\n",
      "Epoch [13][15]\t Batch [10][550]\t Training Loss 1.8876\t Accuracy 0.3473\n",
      "Epoch [13][15]\t Batch [20][550]\t Training Loss 1.9117\t Accuracy 0.3381\n",
      "Epoch [13][15]\t Batch [30][550]\t Training Loss 1.9233\t Accuracy 0.3348\n",
      "Epoch [13][15]\t Batch [40][550]\t Training Loss 1.9221\t Accuracy 0.3400\n",
      "Epoch [13][15]\t Batch [50][550]\t Training Loss 1.9179\t Accuracy 0.3386\n",
      "Epoch [13][15]\t Batch [60][550]\t Training Loss 1.9148\t Accuracy 0.3379\n",
      "Epoch [13][15]\t Batch [70][550]\t Training Loss 1.9155\t Accuracy 0.3392\n",
      "Epoch [13][15]\t Batch [80][550]\t Training Loss 1.9152\t Accuracy 0.3384\n",
      "Epoch [13][15]\t Batch [90][550]\t Training Loss 1.9172\t Accuracy 0.3389\n",
      "Epoch [13][15]\t Batch [100][550]\t Training Loss 1.9100\t Accuracy 0.3423\n",
      "Epoch [13][15]\t Batch [110][550]\t Training Loss 1.9092\t Accuracy 0.3432\n",
      "Epoch [13][15]\t Batch [120][550]\t Training Loss 1.9082\t Accuracy 0.3423\n",
      "Epoch [13][15]\t Batch [130][550]\t Training Loss 1.9059\t Accuracy 0.3427\n",
      "Epoch [13][15]\t Batch [140][550]\t Training Loss 1.9069\t Accuracy 0.3431\n",
      "Epoch [13][15]\t Batch [150][550]\t Training Loss 1.9062\t Accuracy 0.3438\n",
      "Epoch [13][15]\t Batch [160][550]\t Training Loss 1.9061\t Accuracy 0.3440\n",
      "Epoch [13][15]\t Batch [170][550]\t Training Loss 1.9055\t Accuracy 0.3436\n",
      "Epoch [13][15]\t Batch [180][550]\t Training Loss 1.9063\t Accuracy 0.3432\n",
      "Epoch [13][15]\t Batch [190][550]\t Training Loss 1.9070\t Accuracy 0.3434\n",
      "Epoch [13][15]\t Batch [200][550]\t Training Loss 1.9066\t Accuracy 0.3431\n",
      "Epoch [13][15]\t Batch [210][550]\t Training Loss 1.9052\t Accuracy 0.3433\n",
      "Epoch [13][15]\t Batch [220][550]\t Training Loss 1.9038\t Accuracy 0.3437\n",
      "Epoch [13][15]\t Batch [230][550]\t Training Loss 1.9039\t Accuracy 0.3437\n",
      "Epoch [13][15]\t Batch [240][550]\t Training Loss 1.9033\t Accuracy 0.3437\n",
      "Epoch [13][15]\t Batch [250][550]\t Training Loss 1.9028\t Accuracy 0.3439\n",
      "Epoch [13][15]\t Batch [260][550]\t Training Loss 1.9024\t Accuracy 0.3438\n",
      "Epoch [13][15]\t Batch [270][550]\t Training Loss 1.9002\t Accuracy 0.3444\n",
      "Epoch [13][15]\t Batch [280][550]\t Training Loss 1.8999\t Accuracy 0.3451\n",
      "Epoch [13][15]\t Batch [290][550]\t Training Loss 1.8998\t Accuracy 0.3451\n",
      "Epoch [13][15]\t Batch [300][550]\t Training Loss 1.8992\t Accuracy 0.3452\n",
      "Epoch [13][15]\t Batch [310][550]\t Training Loss 1.8996\t Accuracy 0.3449\n",
      "Epoch [13][15]\t Batch [320][550]\t Training Loss 1.9010\t Accuracy 0.3445\n",
      "Epoch [13][15]\t Batch [330][550]\t Training Loss 1.9013\t Accuracy 0.3443\n",
      "Epoch [13][15]\t Batch [340][550]\t Training Loss 1.9016\t Accuracy 0.3443\n",
      "Epoch [13][15]\t Batch [350][550]\t Training Loss 1.9007\t Accuracy 0.3447\n",
      "Epoch [13][15]\t Batch [360][550]\t Training Loss 1.9001\t Accuracy 0.3449\n",
      "Epoch [13][15]\t Batch [370][550]\t Training Loss 1.9005\t Accuracy 0.3451\n",
      "Epoch [13][15]\t Batch [380][550]\t Training Loss 1.9002\t Accuracy 0.3446\n",
      "Epoch [13][15]\t Batch [390][550]\t Training Loss 1.9000\t Accuracy 0.3447\n",
      "Epoch [13][15]\t Batch [400][550]\t Training Loss 1.9006\t Accuracy 0.3445\n",
      "Epoch [13][15]\t Batch [410][550]\t Training Loss 1.9012\t Accuracy 0.3442\n",
      "Epoch [13][15]\t Batch [420][550]\t Training Loss 1.9013\t Accuracy 0.3441\n",
      "Epoch [13][15]\t Batch [430][550]\t Training Loss 1.9017\t Accuracy 0.3440\n",
      "Epoch [13][15]\t Batch [440][550]\t Training Loss 1.9017\t Accuracy 0.3440\n",
      "Epoch [13][15]\t Batch [450][550]\t Training Loss 1.9015\t Accuracy 0.3443\n",
      "Epoch [13][15]\t Batch [460][550]\t Training Loss 1.9015\t Accuracy 0.3439\n",
      "Epoch [13][15]\t Batch [470][550]\t Training Loss 1.9019\t Accuracy 0.3438\n",
      "Epoch [13][15]\t Batch [480][550]\t Training Loss 1.9019\t Accuracy 0.3432\n",
      "Epoch [13][15]\t Batch [490][550]\t Training Loss 1.9031\t Accuracy 0.3427\n",
      "Epoch [13][15]\t Batch [500][550]\t Training Loss 1.9020\t Accuracy 0.3437\n",
      "Epoch [13][15]\t Batch [510][550]\t Training Loss 1.9020\t Accuracy 0.3440\n",
      "Epoch [13][15]\t Batch [520][550]\t Training Loss 1.9024\t Accuracy 0.3438\n",
      "Epoch [13][15]\t Batch [530][550]\t Training Loss 1.9014\t Accuracy 0.3442\n",
      "Epoch [13][15]\t Batch [540][550]\t Training Loss 1.9010\t Accuracy 0.3445\n",
      "\n",
      "Epoch [13]\t Average training loss 1.9016\t Average training accuracy 0.3442\n",
      "Epoch [13]\t Average validation loss 1.9070\t Average validation accuracy 0.3414\n",
      "\n",
      "Epoch [14][15]\t Batch [0][550]\t Training Loss 2.0712\t Accuracy 0.2600\n",
      "Epoch [14][15]\t Batch [10][550]\t Training Loss 1.9311\t Accuracy 0.3464\n",
      "Epoch [14][15]\t Batch [20][550]\t Training Loss 1.9160\t Accuracy 0.3462\n",
      "Epoch [14][15]\t Batch [30][550]\t Training Loss 1.9233\t Accuracy 0.3400\n",
      "Epoch [14][15]\t Batch [40][550]\t Training Loss 1.9100\t Accuracy 0.3390\n",
      "Epoch [14][15]\t Batch [50][550]\t Training Loss 1.9078\t Accuracy 0.3439\n",
      "Epoch [14][15]\t Batch [60][550]\t Training Loss 1.9096\t Accuracy 0.3395\n",
      "Epoch [14][15]\t Batch [70][550]\t Training Loss 1.9087\t Accuracy 0.3394\n",
      "Epoch [14][15]\t Batch [80][550]\t Training Loss 1.9068\t Accuracy 0.3437\n",
      "Epoch [14][15]\t Batch [90][550]\t Training Loss 1.9039\t Accuracy 0.3440\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14][15]\t Batch [100][550]\t Training Loss 1.9074\t Accuracy 0.3417\n",
      "Epoch [14][15]\t Batch [110][550]\t Training Loss 1.9057\t Accuracy 0.3407\n",
      "Epoch [14][15]\t Batch [120][550]\t Training Loss 1.9066\t Accuracy 0.3412\n",
      "Epoch [14][15]\t Batch [130][550]\t Training Loss 1.9032\t Accuracy 0.3418\n",
      "Epoch [14][15]\t Batch [140][550]\t Training Loss 1.9047\t Accuracy 0.3413\n",
      "Epoch [14][15]\t Batch [150][550]\t Training Loss 1.9040\t Accuracy 0.3408\n",
      "Epoch [14][15]\t Batch [160][550]\t Training Loss 1.9051\t Accuracy 0.3400\n",
      "Epoch [14][15]\t Batch [170][550]\t Training Loss 1.9034\t Accuracy 0.3416\n",
      "Epoch [14][15]\t Batch [180][550]\t Training Loss 1.9018\t Accuracy 0.3423\n",
      "Epoch [14][15]\t Batch [190][550]\t Training Loss 1.9019\t Accuracy 0.3409\n",
      "Epoch [14][15]\t Batch [200][550]\t Training Loss 1.9021\t Accuracy 0.3409\n",
      "Epoch [14][15]\t Batch [210][550]\t Training Loss 1.9038\t Accuracy 0.3410\n",
      "Epoch [14][15]\t Batch [220][550]\t Training Loss 1.9033\t Accuracy 0.3416\n",
      "Epoch [14][15]\t Batch [230][550]\t Training Loss 1.9031\t Accuracy 0.3422\n",
      "Epoch [14][15]\t Batch [240][550]\t Training Loss 1.9025\t Accuracy 0.3418\n",
      "Epoch [14][15]\t Batch [250][550]\t Training Loss 1.9040\t Accuracy 0.3410\n",
      "Epoch [14][15]\t Batch [260][550]\t Training Loss 1.9054\t Accuracy 0.3406\n",
      "Epoch [14][15]\t Batch [270][550]\t Training Loss 1.9043\t Accuracy 0.3411\n",
      "Epoch [14][15]\t Batch [280][550]\t Training Loss 1.9035\t Accuracy 0.3411\n",
      "Epoch [14][15]\t Batch [290][550]\t Training Loss 1.9037\t Accuracy 0.3403\n",
      "Epoch [14][15]\t Batch [300][550]\t Training Loss 1.9035\t Accuracy 0.3411\n",
      "Epoch [14][15]\t Batch [310][550]\t Training Loss 1.9021\t Accuracy 0.3414\n",
      "Epoch [14][15]\t Batch [320][550]\t Training Loss 1.9021\t Accuracy 0.3421\n",
      "Epoch [14][15]\t Batch [330][550]\t Training Loss 1.9019\t Accuracy 0.3426\n",
      "Epoch [14][15]\t Batch [340][550]\t Training Loss 1.9013\t Accuracy 0.3431\n",
      "Epoch [14][15]\t Batch [350][550]\t Training Loss 1.9012\t Accuracy 0.3428\n",
      "Epoch [14][15]\t Batch [360][550]\t Training Loss 1.9018\t Accuracy 0.3427\n",
      "Epoch [14][15]\t Batch [370][550]\t Training Loss 1.9028\t Accuracy 0.3420\n",
      "Epoch [14][15]\t Batch [380][550]\t Training Loss 1.9025\t Accuracy 0.3421\n",
      "Epoch [14][15]\t Batch [390][550]\t Training Loss 1.9024\t Accuracy 0.3421\n",
      "Epoch [14][15]\t Batch [400][550]\t Training Loss 1.9028\t Accuracy 0.3419\n",
      "Epoch [14][15]\t Batch [410][550]\t Training Loss 1.9033\t Accuracy 0.3416\n",
      "Epoch [14][15]\t Batch [420][550]\t Training Loss 1.9022\t Accuracy 0.3424\n",
      "Epoch [14][15]\t Batch [430][550]\t Training Loss 1.9041\t Accuracy 0.3419\n",
      "Epoch [14][15]\t Batch [440][550]\t Training Loss 1.9043\t Accuracy 0.3420\n",
      "Epoch [14][15]\t Batch [450][550]\t Training Loss 1.9048\t Accuracy 0.3414\n",
      "Epoch [14][15]\t Batch [460][550]\t Training Loss 1.9034\t Accuracy 0.3418\n",
      "Epoch [14][15]\t Batch [470][550]\t Training Loss 1.9032\t Accuracy 0.3419\n",
      "Epoch [14][15]\t Batch [480][550]\t Training Loss 1.9025\t Accuracy 0.3423\n",
      "Epoch [14][15]\t Batch [490][550]\t Training Loss 1.9026\t Accuracy 0.3421\n",
      "Epoch [14][15]\t Batch [500][550]\t Training Loss 1.9026\t Accuracy 0.3420\n",
      "Epoch [14][15]\t Batch [510][550]\t Training Loss 1.9029\t Accuracy 0.3418\n",
      "Epoch [14][15]\t Batch [520][550]\t Training Loss 1.9025\t Accuracy 0.3419\n",
      "Epoch [14][15]\t Batch [530][550]\t Training Loss 1.9036\t Accuracy 0.3416\n",
      "Epoch [14][15]\t Batch [540][550]\t Training Loss 1.9035\t Accuracy 0.3414\n",
      "\n",
      "Epoch [14]\t Average training loss 1.9041\t Average training accuracy 0.3412\n",
      "Epoch [14]\t Average validation loss 1.8969\t Average validation accuracy 0.3422\n",
      "\n",
      "Testing...\n",
      "The test accuracy is 0.0816.\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEGCAYAAABo25JHAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAArsElEQVR4nO3dd3hc1Z3/8fdX3dJIY9mSiyQb2WBjW3JFlMUUEyDUpQRY42UDhmyALAktAUIazpJkyQ+nLLuhOLCQ4sSQ0AndAQwhENxt2YBxActVclGzus7vjxkJWZ4ZjWSNRqP5vJ5nHo3unHvnKz22PnPuOfdcc84hIiLxKyHaBYiISHQpCERE4pyCQEQkzikIRETinIJARCTOJUW7gO7KyclxhYWF0S5DRCSmLFu2rMI5lxvotZgLgsLCQpYuXRrtMkREYoqZfRrsNZ0aEhGJcwoCEZE4pyAQEYlzMTdGICIDR1NTE2VlZdTX10e7lAEjLS2NgoICkpOTw95HQSAiUVNWVkZmZiaFhYWYWbTLiXnOOfbs2UNZWRljxowJez+dGhKRqKmvr2fo0KEKgV5iZgwdOrTbPSwFgYhElUKgd/Xk96kgEBGJcwoCEYlbe/bsYdq0aUybNo0RI0aQn5/f/n1jY2PIfZcuXcqNN97Y5XuceOKJvVVuxGiwWERiQsmPXqOi5tA/zjmeFJZ+78weHXPo0KGsXLkSgHnz5uHxePjWt77V/npzczNJSYH/TJaUlFBSUtLle7z77rs9qq0vqUcgIjEhUAiE2t5Tc+fO5dZbb+W0007jjjvu4B//+Acnnngi06dP58QTT+Sjjz4C4M033+T8888HfCFyzTXXMGvWLMaOHct9993XfjyPx9PeftasWVx66aVMmDCBK664grY7RL744otMmDCBk046iRtvvLH9uH1FPQIR6Rd++Hwp67ZX9Wjf2Q/9PeD2SXlZ3PXPRd0+3scff8zrr79OYmIiVVVVLFmyhKSkJF5//XW+853v8OSTTx6yz4cffsgbb7xBdXU1Rx99NF/72tcOmcu/YsUKSktLycvLY+bMmfztb3+jpKSE6667jiVLljBmzBjmzJnT7XoPl4JARKSTyy67jMTERAAqKyu56qqr2LBhA2ZGU1NTwH3OO+88UlNTSU1NZdiwYezatYuCgoKD2hx33HHt26ZNm8aWLVvweDyMHTu2fd7/nDlzWLBgQQR/ukMpCESkX+jqk3vht/8S9LXHr/unXq0lIyOj/fn3v/99TjvtNJ5++mm2bNnCrFmzAu6Tmpra/jwxMZHm5uaw2rSdHoomjRGIiIRQWVlJfn4+AI899livH3/ChAls2rSJLVu2APD444/3+nt0RUEgIjEhx5PSre295fbbb+fOO+9k5syZtLS09PrxBw0axP3338/ZZ5/NSSedxPDhw/F6vb3+PqFYf+iWdEdJSYnTjWlEBob169czceLEaJcRdTU1NXg8Hpxz3HDDDYwbN45bbrmlx8cL9Hs1s2XOuYDzXeN+jCASc5NFRLrj17/+Nb/5zW9obGxk+vTpXHfddX36/nEfBH01N1lEJJhbbrnlsHoAh0tjBCIicU5BICIS5xQEIiJxTkEgIhLn4j4IojU3WUSib9asWbzyyisHbfvlL3/Jf/zHfwRt3zZ9/dxzz2X//v2HtJk3bx7z588P+b7PPPMM69ata//+Bz/4Aa+//no3q+89cT9rqG2K6J+WbuW2P6/m9VtP5ahhnihXJSKHuHcc1O4+dHvGMLhtQ48OOWfOHBYtWsRZZ53Vvm3RokXce++9Xe774osv9ug9wRcE559/PpMmTQLgP//zP3t8rN4Q9z2CNsX5viv5SrdXRrkSEQkoUAiE2h6GSy+9lBdeeIGGhgYAtmzZwvbt2/nDH/5ASUkJRUVF3HXXXQH3LSwspKKiAoAf//jHHH300Zxxxhnty1SD7/qAY489lqlTp3LJJZdw4MAB3n33XZ577jluu+02pk2bxsaNG5k7dy5//vOfAVi8eDHTp09n8uTJXHPNNe21FRYWctdddzFjxgwmT57Mhx9+2OOfu7O47xG0GTfMQ2pSAmu3VXLhtPxolyMSf176Nuxc07N9Hz0v8PYRk+Gce4LuNnToUI477jhefvllLrzwQhYtWsTs2bO58847GTJkCC0tLZx++umsXr2aKVOmBDzGsmXLWLRoEStWrKC5uZkZM2ZwzDHHAPClL32Jr371qwB873vf45FHHuEb3/gGF1xwAeeffz6XXnrpQceqr69n7ty5LF68mPHjx3PllVfywAMPcPPNNwOQk5PD8uXLuf/++5k/fz4PP/xwN39RgalH4JeUmMCEkVms3daz9dBFJDa1nR4C32mhOXPm8MQTTzBjxgymT59OaWnpQefzO3v77be5+OKLSU9PJysriwsuuKD9tbVr13LyySczefJkFi5cSGlpachaPvroI8aMGcP48eMBuOqqq1iyZEn761/60pcAOOaYY9oXqesN6hF0UJyXxXOrtuOcw8yiXY5IfAnxyR2AeSEWYrs6+BLVXbnooou49dZbWb58OXV1dWRnZzN//nw++OADsrOzmTt3LvX19SGPEezvxdy5c3nmmWeYOnUqjz32GG+++WbI43S19lvbMtbBlrnuKfUIOijO91Jd38zWvXXRLkVE+ojH42HWrFlcc801zJkzh6qqKjIyMvB6vezatYuXXnop5P6nnHIKTz/9NHV1dVRXV/P888+3v1ZdXc3IkSNpampi4cKF7dszMzOprq4+5FgTJkxgy5YtfPLJJwD87ne/49RTT+2lnzQ4BUEHxXm+TxxrNWAs0v9kDOve9m6YM2cOq1at4vLLL2fq1KlMnz6doqIirrnmGmbOnBly3xkzZjB79mymTZvGJZdcwsknn9z+2t13383xxx/PmWeeyYQJE9q3X3755dx7771Mnz6djRs3tm9PS0vj0Ucf5bLLLmPy5MkkJCRw/fXXH/bP1xUtQ91BQ3MLRT94hWtPGcvtZ0/oegcROSxahjoyursMtXoEHaQmJTJ+eCZre3gDbRGRWKQg6KQ4P4vSbZX94j6iIiJ9QUHQSXG+lz21jeyoDD1LQER6hz509a6e/D4VBJ0UtQ0Yb9OAsUikpaWlsWfPHoVBL3HOsWfPHtLS0rq1n64j6GTiyEwSDNZur+KLRSOiXY7IgFZQUEBZWRnl5eXRLmXASEtLo6CgoFv7KAg6SU9J4shcD6XqEYhEXHJyMmPGjIl2GXFPp4YCmJzv1bUEIhI3IhYEZjbKzN4ws/VmVmpmNwVpN8vMVvrbvBWperqjKN/LrqoGdldrwFhEBr5InhpqBr7pnFtuZpnAMjN7zTnXvnqTmQ0G7gfOds59ZmaHf4lgLyjOywKgdHsVw47u3qCLiEisiViPwDm3wzm33P+8GlgPdF7f+V+Bp5xzn/nb9Xxh8V40qS0INE4gInGgT8YIzKwQmA683+ml8UC2mb1pZsvM7Mog+19rZkvNbGlfzC7ITEtmTE6GlqQWkbgQ8SAwMw/wJHCzc67zX9Yk4BjgPOAs4PtmNr7zMZxzC5xzJc65ktzc3EiXDEBRXpYGjEUkLkQ0CMwsGV8ILHTOPRWgSRnwsnOu1jlXASwBpkaypnAV53sp21fH/gON0S5FRCSiIjlryIBHgPXOuZ8HafYscLKZJZlZOnA8vrGEqGtbkrpUC9CJyAAXyVlDM4EvA2vMbKV/23eA0QDOuQedc+vN7GVgNdAKPOycWxvBmsJW5B8wXrOtkplH5US5GhGRyIlYEDjn3gG6vN+jc+5e4N5I1dFT2Rkp5A8epDWHRGTA05XFIRTnZ+nUkIgMeAqCEIrzvGyuqKW6vinapYiIRIyCIITiAt+A8Tr1CkRkAFMQhPD5zewVBCIycCkIQsjNTGV4VqqWmhCRAU1B0IXiPC1JLSIDm4KgC0X5Xj7ZXUNdY0u0SxERiQgFQReK87JodbB+p8YJRGRgUhB0oTjfv9SExglEZIBSEHRhpDeNIRkpWpJaRAYsBUEXzExLUovIgKYgCENxvpePd1XT0KwBYxEZeBQEYSjO89LU4vh4Z020SxER6XUKgjAU5/uWpNbpIREZiBQEYRg9JJ3MtCQtSS0iA5KCIAxm5r/CWDOHRGTgURCEqTg/i/U7qmhqaY12KSIivUpBEKbifC+Nza1sLNeAsYgMLAqCMBW1LUmtC8tEZIBREIRpTE4G6SmJGjAWkQFHQRCmxARj0sgsSjWFVEQGGAVBNxTneyndXkVrq4t2KSIivUZB0A1FeVkcaGxh857aaJciItJrFATd0LYktcYJRGQgURB0w1HDPKQkJSgIRGRAURB0Q3JiAhNHZGoKqYgMKAqCbirO993M3jkNGIvIwKAg6KbifC/V9c1s3VsX7VJERHqFgqCbituuMNb1BCIyQCgIumn8CA9JCaYBYxEZMBQE3ZSalMj44ZlaklpEBgwFQQ8U52dRuk0DxiIyMCgIeqA438ue2kZ2VtVHuxQRkcOmIOgBLUktIgNJxILAzEaZ2Rtmtt7MSs3sphBtjzWzFjO7NFL19KaJIzNJMC01ISIDQ1IEj90MfNM5t9zMMoFlZvaac25dx0Zmlgj8FHglgrX0qvSUJI7M9WhJahEZECLWI3DO7XDOLfc/rwbWA/kBmn4DeBLYHalaIqE438sa9QhEZADokzECMysEpgPvd9qeD1wMPNjF/tea2VIzW1peXh6xOrujKC+LXVUN7K7WgLGIxLaIB4GZefB94r/ZOdd5dPWXwB3OuZZQx3DOLXDOlTjnSnJzcyNUafe0LUldqusJRCTGRTQIzCwZXwgsdM49FaBJCbDIzLYAlwL3m9lFkayptxTlZQFQqtNDIhLjIjZYbGYGPAKsd879PFAb59yYDu0fA15wzj0TqZp6U2ZaMmNyMjSFVERiXiRnDc0EvgysMbOV/m3fAUYDOOdCjgvEgqK8LFZu3R/tMkREDkvEgsA59w5g3Wg/N1K1REpxvpcXVu9g/4FGBqenRLscEZEe0ZXFh6FtSWoNGItILFMQHIa2AWNdYSwisUxBcBiyM1LIHzxIS1KLSExTEBymtiWpRURilYLgMBXnedlUUUt1fVO0SxER6REFwWFqu8J4/Y7qKFciItIzCoLDVJTvGzDWAnQiEqsUBIdpWGYawzJTNU4gIjErrCAwswwzS/A/H29mF/jXERJgcr6Xtbo3gYjEqHB7BEuANP+y0YuBq4HHIlVUrCnK9/LJ7hrqGkMuoioi0i+FGwTmnDsAfAn4H+fcxcCkyJUVW4rzsmh1sH6nricQkdgTdhCY2T8BVwB/8W+L5IJ1MaX93gQaJxCRGBRuENwM3Ak87ZwrNbOxwBsRqyrGjPSmMSQjRUtSi0hMCutTvXPuLeAtAP+gcYVz7sZIFhZLzIyivCwNGItITAp31tAfzCzLzDKAdcBHZnZbZEuLLcX5Xj7eVU1DswaMRSS2hHtqaJL/fsMXAS/iu7nMlyNVVCwqzvPS1OLYsKsm2qWIiHRLuEGQ7L9u4CLgWedcE+AiVlUMKs7XktQiEpvCDYKHgC1ABrDEzI4ANDLawegh6WSmJWmcQERiTriDxfcB93XY9KmZnRaZkmJT+4CxZg6JSIwJd7DYa2Y/N7Ol/sfP8PUOpIPiPC/rd1TR3NIa7VJERMIW7qmh/wOqgX/xP6qARyNVVKyaXOClobmVT8o1YCwisSPcq4OPdM5d0uH7H5rZygjUE9OK/DezX7utigkjsqJcjYhIeMLtEdSZ2Ult35jZTKAuMiXFrjE5GaSnJGrmkIjElHB7BNcDvzUzr//7fcBVkSkpdiUmGJNGZlGqmUMiEkPCnTW0CphqZln+76vM7GZgdQRr6xv3joPa3YduzxgGt23o9uGK8708sXQrra2OhATrhQJFRCKrW3coc85V+a8wBrg1AvX0vUAhEGp7F4rysjjQ2MLmPbWHUZSISN85nFtV6uNuAG1LUmucQERixeEEgZaYCOCoYR5SkhIo3a4Ly0QkNoQcIzCzagL/wTdgUEQqinHJiQlMHJGpHoGIxIyQQeCcy+yrQgaSonwvL6zajnMOM51BE5H+7XBODQ0MGcOCbM/p8SGL87xU1TdTtk+XWohI/6f7DneeIrr1A3j0HMibAa2tkND9rGxbknrNtkpGDUnvjSpFRCJGPYLORh0L59wDG16Ft+f36BBHj8gkKcE0TiAiMUFBEEjJV2DqHHjjJ7Dh9W7vnpqUyPjhmazVzCERiQERCwIzG2Vmb5jZejMrNbObArS5wsxW+x/vmtnUSNXTLWZw3s9heDE8+RXYt6XbhyjOz6J0WyXOaZatiPRvkewRNAPfdM5NBE4AbjCzSZ3abAZOdc5NAe4GFkSwnu5JSYfZvwUcPHElNHVv4Lc438ue2kZ2VtVHpj4RkV4SsSBwzu1wzi33P68G1gP5ndq865zb5//2PaAgUvX0yJCxcPEC2LEK/vIt6Man+45LUouI9Gd9MkZgZoXAdOD9EM2+ArwUZP9r2+6OVl5eHoEKQzj6bDjldlj5e1j+m7B3mzgykwTTUhMi0v9FPAjMzAM8CdzcYcG6zm1OwxcEdwR63Tm3wDlX4pwryc3NjVyxwcz6Nhx5Orx4G2xbFtYu6SlJHJnr0ZLUItLvRTQIzCwZXwgsdM49FaTNFOBh4ELn3J5I1tNjCYlwycPgGQFPXAW14ZVZnO/VqSER6fcsUrNazLe2wm+Avc65m4O0GQ38FbjSOfduOMctKSlxS5cu7bU6u2X7CnjkLDjiRPi3J30BEUTJj16joqbxkO05nhSWfu/MSFYpInIIM1vmnCsJ9FokewQzgS8DXzCzlf7HuWZ2vZld72/zA2AocL//9Sj9hQ9T3nQ472ew6Q3fNQYhBAqBUNtFRKIlYktMOOfeoYt7Fjjn/h3490jVEBEzvgxlH/iuOs4/BiacG+2KREQOi64s7olz/h+MnAZPXwd7Nka7GhGRw6Ig6InkNJj9O98YweNfhkbdllJEYpeCoKcGj4ZLHoHd6+D5m7p1sdmGXdURLExEpHsUBIfjqNPhC9+FNX+Cf/z6oJdyPCkBdzHgol/9jZfX7uiDAkVEuhax6aOREtXpo4G0tsKif4VPXoO5L8Lo40M231FZx/W/X86qrfu54bQjufXMo0lM0F3MRCSyojV9ND4kJMDFD4J3FPzpKqjZHbL5SO8gHr/2BGaXjOJXb2zkmsc+oPJAUx8VKyJyKAVBbxg0GGb/Hur2w5+uhpbmkM3TkhO555LJ/PjiYt7dWMEFv3qHD3fqCmQRiQ4FQW8ZUQz//N/w6TuweF6Xzc2MK44/gkXXnsCBxhYu/tW7vLB6e+TrFBHpREHQm6bOhmO/Cu/+D5Q+E9YuxxwxhBe+cRITR2by9T+s4L9eWk9zS2tk6xQR6UBB0NvO+gkUHAvP3gDlH4W1y/CsNBZd+09ccfxoHnprE3Mf/YB9tVqKQkT6hmYNRULlNvhFERDgd5sxDG7bEHTXxz/4jO8/U8qwrFQe+vIx7Te4ERE5HJo11Ne8+QQMAYDa0LOKZh87msevO4HmFsclD7zLsyu39X59IiIdKAj6oemjs3n+GycxJX8wNy1ayd0vrNO4gYhEjIKgn8rNTGXhV49n7omFPPLOZv7tkffZU9MQ7bJEZABSEETDmj/7rkjuQnJiAvMuKGL+ZVNZ/tl+/vl/3mFNmW59KSK9S4PFkTKvi0HevBnwxR9B4cywDremrJLrf7+MbfvrAr6uO5+JSCgaLI6GjGHBt1/0ANTsgsfOhT/+K1QEn0XUZnKBl+e+Hjw0dOczEempiN2hLO6FmCIKQNHF8N798PYv4FfHQ8nVcOq3wZMbdJehntReLlJERD2C6EkeBCd/E25cASXXwNJH4b7psGQ+NB6IdnUiEkcUBNHmyYXz5sMN78PYU+Gvd8P/HAMrFkJrS7cOdfmCv/Pepj0RKlREBioFQX+RMw4uXwhXvwSZI+DZ/4CHToWNfw37EBvLa7l8wXvMWfAe7ysQRCRMCoL+5ogT4d8X+26D2VAJv7sYfn8J7CoFgt/5LMeTwtu3n8YPzp/EJ+U1zF7wHv/66/f4YMvevqxeRGKQpo/2Z80NvltgLvl/0FAN066Aj16CAxWHtu2whlF9Uwu/f+9THnxrExU1Dcw8aii3nDGeksIhffwDiEh/EWr6qIIgFhzYC2//DN5/CFpD3M1s3sEXm9U1trDw/U958K2NVNQ0cvK4HG4+YxzHHKFAEIk3CoKBYu9muG9a8NfnBb7quK6xrYewkT21bYEwnmOOyI5MnSLS7+iCsoFiyJge7TYoJZGvnjKWt+84je+cO4F126u45IF3ufL//sHyz/b1cpEiEmvUI4g1oZauGDkNjpkLky+F1MygzQ40NvPbv3/KgiWb2FvbyKnjc1m1dT/76w497aSlK0QGBvUI4kVLE7xwM/xsAjx/M+xYFbBZekoS1596JG/ffhp3nD2B1WWBQwC0dIVIPFAQxJpQaxh97W/wlddg4gWw6o/w0Cmw4DRY/ltorD10l9QkvjbrSN654wsRLlpE+jOdGhqo6vbBqsdh2aNQ/iGkZMKUf/GtaTRi8iHNC7/9l6CH+uklkzlj4nCtdSQSwzRrKJ45B1vf961lVPo0tDRAfokvEIouhpQMIHQQACQYlBQO4ayiEZxVNJyC7PS+qF5EeomCQHwO7IVVi3y9hIqPIdXb3ksov/8ccu3Q6aflzsvu69bwSukuXi3dyYc7qwEozs/irEkjOKt4BOOGeTCzvv5pRKQbFARyMOfgs7/7egnrnvX1EkLpcH3ClopaXindySulO1n+2X4AxuRktPcUphYMJiFBoSDS3ygIJLgDe2HlH+DV7wZvE+RCtV1V9by6ztdT+PvGPTS3OoZnpfLFSSM4q2gEx48dQnJiAiU/ei3g7CNNTRXpOwoC6Vqo6xPOnQ/jzoTswqBNKg808dePdvHK2l28+fFu6pta8Q5K5vSJw3hq+bag+22557zDKFpEwhUqCCJ2hzIzGwX8FhgBtAILnHP/3amNAf8NnAscAOY655ZHqibpoRe/5fs69Cg46gzf44iZkPL5gLE3PZmLpxdw8fQC6hpbWLKhnFdKd7J4/e4oFS0i4YrkrSqbgW8655abWSawzMxec86t69DmHGCc/3E88ID/q/QnX18Gn7zueyx7DN5/EJLSfGHQFgw548A/YDwoJdE/ZjCCppZWxn33paCHbm5pJSlRl7OIRFPEgsA5twPY4X9ebWbrgXygYxBcCPzW+c5PvWdmg81spH9f6UsZw6A2wKf3jGGQc5TvccL10FQHn/4NPlnsC4ZX7vQ9Bo/+PBTGnNK+xEVyF3/kj//JYs6fMpILpuUzY/RgzT4SiYI+GSMws0JgCVDsnKvqsP0F4B7n3Dv+7xcDdzjnlnba/1rgWoDRo0cf8+mnn0a8ZgnTvk9h42LY8DpsfgsaayAhGUaf0B4M5Q+cG3Rq6rzxz/D6+l00NLcyasggLpyaz0XT8zhqWPC1kkSk+6I6WGxmHuAt4MfOuac6vfYX4L86BcHtzrllwY6nweJ+rLnRd/Fa22mkXWu73mdeJdX1TbxSuotnV27jb59U0Opg0sgsLpyWxwXT8hjpHRT52kUGuKgFgZklAy8Arzjnfh7g9YeAN51zf/R//xEwK9SpIQVBDKna4estPHtD8DadpqaWVzfwwurtPLtyOyu37scMjiscwkXT8zmneASD0wPfqlNEQotKEPhnBP0G2OucuzlIm/OAr+ObNXQ8cJ9z7rhQx1UQxKBQU1NLroEps2HU8e2DzW22VNTy3KrtPLNyG5vKa0lONGYdPYwLp+VxxsThnPTTv+r6BJEwRSsITgLeBtbgmz4K8B1gNIBz7kF/WPwvcDa+6aNXdx4f6ExBEINCBUHSIGiu8w02T77MFwq5Rx/UxDlH6fYqnlmxjedXb2dXVQOe1CRqGpqDHlbXJ4gcTBeUSXSFCoI7y+DDv8DqJ2DTG+BaYcQUXyAUXwJZIw9q3tLqeH/THp5duZ3Hl24NelgFgcjBFAQSXfeOCz419bYNn39fvQtKn4LVj8P2FYDB2FNh8r/AxH+GtKyDdg+1YmrJEdlMKRjM1FFephQMpnBouqamSlxTEEjsqdgAa/7kC4V9W3wXsB19ji8UjjoDklK6DIK12yupb/KdlcxKS2JKwWCmFHjbv470pikcJG4oCCR2OQdlS32BUPoUHNgDg7Kh6GL2ffA42XbondfKnZfcH35Gc0srG3bXsLpsP6vKKlldtp8Pd1TT3Or7N5+bmcqUfH8wjPIytWAwQzJStEieDEgKAhkYWppg4xu+UPjwL75B5mC+swOSBx0yE6m+qYX1O6pYXVbJqrL9rC6rZGN5DW3/DQqyB/H0gblBL4DL/eFnvfkTifSZqCw6J9LrEpNh/Bd9j4Zq+K+C4G1/MhIwSPFAqsd3J7aUDNJSMpmeksH0lAxI88AkDw0Jg9hZn8RnNcamSsitC7zsdq5V8uslmxiTk8GY3AxGZaeTkqR1kiT2KQgkNqV2sQTFGT/0LXfRWOv72tD2vBZqdn7+vKGG1MYajsBxBHByF2+b9eotLHfDecoNZ6uNoNlbyPDcXAqHZjA2N4MxORkUDs0gb/AgEgPcoEennaQ/UhDIwHTSzeG3dc63mF5jje9x3/SgTS/zriOh9s3PNxyAfZ8NZvOWYWxuHc4/WofzJzeCbQkjcUPGkJMznDG5GYz1B8RLTV8hNy3AaacmL6DTThIdCgIRM9+9FVLSgWEhmybctsHXu9i3GfZugr2byN67icF7NzG1YiOJNW9/3rgKqqoz2fzJMDa74bznRnB8UvDTTk98sJXsjBSGZCSTnZ7CkIwUstKSw7r1p3oacjgUBBK7Qi2dHUmpHhgx2ffwMyARfD2LfVvaQyJr7yam7NlE0Z6NJFa/ByHmZix/5pd86obzWeswdjCUVhJIMMhOT/EFRHoK2RnJDMlIaQ+Ktq+BQgAIul2kIwWBxK6OF6P1psMJmORBMGyi7+Fn+P+jNTfAj4If457kh9uftyQkU5OWx56UfHYljqTMhrO5cRgbanJZUj+EXQdonwYL8EHq14LOdHph9XscmethTE4GacmJXf8MEncUBCKdRSpgklJDv37zGti7GfZtJnHvZrz7NuPdu5mxe1+FxuqDmrqhebQMLqTOM5qq9FHkLgt+yunrf1gB+M6A5Q8exNhcD0fmZrR/PTLXw7DM1IAX1+mUU3xQEIj0F4NH+x6cevB253wX0vlDgr2bsX2bSdq7mcytb5BZsyvkYVdMe46KVi/bm9LZUp/Bx/vSWL45hReaMthHJs0k4UlNYqw/FMbmZHDkMA9jcyM7uB2pkKmYN5ocDq25Ai858/rpgHy4y7BEiIJApC/15LSTGWTk+B6jjj309cZa+Ele0N2zy94gu7aCca7l84hJ9D+A+mQv1QmD2VOVyY69HsrWePiYLP7usrg7OXhP49mV28hMSyIjJQlPWhKZqclkpCbiSUsiNanrU1CRCplAIRBqe78Q6N9EqO1h6hi2KSOOOiZYOwWBSF+KxKe7lIzQr3/rY2hthfr9UFsBteVwwP+1toK02nLSaivIra1gQm05rvZjqNuHhRrZBiY89UWqSKfKZbCddKpcOlVkUOXSqbUMmpIzaUrJoiU5C9K8uDQvCYO8ZAxKIyMlie8FGNMAX8g89NZGmlsdLa2O1uZGrKmWxOY6rLmOxOYDJDYfIKG5jsSWOpJbfF+TmutIbq3jwhA1v/v0/aR4hpKeNZQM71CysnPIys4lMSUt9O+Qw+hptDT7fvd1++DAXqjbe+jzUFY/AZ7hkDnC9zXNe8gV8wCtrY7GllaaWlppanE0NrceFLYlVhP0LRQEIvEgIQHSh/geueNDNjXw/fGq2wvzxwVtl3/UFPLq9mMNlSQ0VJDYWEVyUzUJbbcfcUCD/9Hhb9AB0qhy6f43CuzCv55OOg0MooFkawnzh+zaiavuDLi9nmRqzENdgoeGpCyaUrJoTfVCmhcblE1yxmDGhehpVL98N87/h93q9pNQv5fE+v0kNuwnuakq4H4ArSRSl5RFyCh/6qudak1hD9mUk81uBrOrdTC7Wr3sdIPZ7Qaz22Wz2w1mHx42B+hxBaIgEBkIensqbWISeELv67nyj4dudM53UV59ZdBHen0l6fX7YcXvgx576LTzsJR0ElI8uJR0LCUdktN9vZ9k/zUfyRn+rx22J6fD3UODHnfXle9Qs7+Cuqo91NfspalmLy0H9uPq9pPQUElSYxUpzVUMqt5JZtVGvNSQSR0JFrp3lPnefKpcOvuch314qHQe9lHAPjeRSjLY5zLZ5zxU4mlvs99lUp8wiOTWJNYlzA567G/m/poc289Qt4+hbi/ZrXvJbtmLt3kPBc3b8TSvIa3l0MUXWywp5HTljhQEIgNBHwwohsXMt/xHaiZ4Q6wFBSGDIPniX/VyYT7Dx05meJhtnXNUNzSztaaeyn17mLJwatC2i85eQUpKCilJCaQmJZKSlMCIpARGJyWQ2v5IJDUp4aA27cuQzAtex89u+Jeui2084Fs6pXpX+9fEmp3wzi/C+lkVBCISXLQu2jsMFXiDn8vvxnHMjKy0ZLLSkiEn9NpWl58wtptVHqzceYOveBvOAVLSYchY36MjBYGIHLZI9jQiFDLBBm67EwJ97ZzkR4JPpe2D91cQiEh09JfTWWHqrZ5GIBG7OC9Y2HaiG9OIiMSBUDem0V01RETinIJARCTOKQhEROKcgkBEJM4pCERE4pyCQEQkzikIRETinIJARCTOKQhEROKcgkBEJM4pCERE4pyCQEQkzikIRETiXMSCwMz+z8x2m9naIK97zex5M1tlZqVmdnWkahERkeAi2SN4DDg7xOs3AOucc1OBWcDPzCwlgvWIiEgAEQsC59wSYG+oJkCmmRng8bdtjlQ9IiISWDTHCP4XmAhsB9YANznnWgM1NLNrzWypmS0tLy/vyxpFRAa8aN6q8ixgJfAF4EjgNTN72zlX1bmhc24BsADAzKrN7KO+LLQX5AAV0S6iG2KtXlDNfSHW6gXV3NERwV6IZhBcDdzjfPfK/MTMNgMTgH90sd9HwW631l+Z2dJYqjnW6gXV3BdirV5QzeGK5qmhz4DTAcxsOHA0sCmK9YiIxKWI9QjM7I/4ZgPlmFkZcBeQDOCcexC4G3jMzNYABtzhnIu1LpyISMyLWBA45+Z08fp24Is9OPSCnlUUVbFWc6zVC6q5L8RavaCaw2K+U/QiIhKvtMSEiEicUxCIiMS5mAoCMzvbzD4ys0/M7NvRricUMxtlZm+Y2Xr/Wko3RbumcJlZopmtMLMXol1LOMxssJn92cw+9P++/ynaNYViZrf4/02sNbM/mllatGvqLNBaYWY2xMxeM7MN/q/Z0ayxsyA13+v/d7HazJ42s8FRLPEgodZjM7NvmZkzs5y+qCVmgsDMEoFfAecAk4A5ZjYpulWF1Ax80zk3ETgBuKGf19vRTcD6aBfRDf8NvOycmwBMpR/Xbmb5wI1AiXOuGEgELo9uVQE9xqFrhX0bWOycGwcs9n/fnzzGoTW/BhQ756YAHwN39nVRITxGgPXYzGwUcCa+KfZ9ImaCADgO+MQ5t8k51wgsAi6Mck1BOed2OOeW+59X4/vjlB/dqrpmZgXAecDD0a4lHGaWBZwCPALgnGt0zu2PalFdSwIGmVkSkI5vmZV+JchaYRcCv/E//w1wUV/W1JVANTvnXnXOta1h9h5Q0OeFBRFiPbZfALfjW4+tT8RSEOQDWzt8X0YM/GEFMLNCYDrwfpRLCccv8f0jDLjuUz80FigHHvWfznrYzDKiXVQwzrltwHx8n/Z2AJXOuVejW1XYhjvndoDvgw4wLMr1dNc1wEvRLiIUM7sA2OacW9WX7xtLQWABtvX7ua9m5gGeBG4OtI5Sf2Jm5wO7nXPLol1LNyQBM4AHnHPTgVr63ymLdv7z6hcCY4A8IMPM/i26VQ18ZvZdfKdrF0a7lmDMLB34LvCDvn7vWAqCMmBUh+8L6Idd6o7MLBlfCCx0zj0V7XrCMBO4wMy24Dv19gUz+310S+pSGVDmnGvrbf0ZXzD0V2cAm51z5c65JuAp4MQo1xSuXWY2EsD/dXeU6wmLmV0FnA9c4fr3hVNH4vuAsMr/f7AAWG5mIyL9xrEUBB8A48xsjP8GNpcDz0W5pqD891l4BFjvnPt5tOsJh3PuTudcgXOuEN/v96/OuX79adU5txPYamZH+zedDqyLYkld+Qw4wczS/f9GTqcfD2538hxwlf/5VcCzUawlLGZ2NnAHcIFz7kC06wnFObfGOTfMOVfo/z9YBszw/xuPqJgJAv+Az9eBV/D9x3nCOVca3apCmgl8Gd+n6pX+x7nRLmqA+gaw0MxWA9OAn0S3nOD8PZc/A8vx3YcjgX64DIJ/rbC/A0ebWZmZfQW4BzjTzDbgm9VyTzRr7CxIzf8LZOJb5n6lmT0Y1SI7CFJvdGrp3z0lERGJtJjpEYiISGQoCERE4pyCQEQkzikIRETinIJARCTOKQhEOjGzlg5Tflf25kq3ZlYYaLVJkWiK2K0qRWJYnXNuWrSLEOkr6hGIhMnMtpjZT83sH/7HUf7tR5jZYv+a94vNbLR/+3D/Gvir/I+2pSQSzezX/nsSvGpmg6L2Q4mgIBAJZFCnU0OzO7xW5Zw7Dt8Vq7/0b/tf4Lf+Ne8XAvf5t98HvOWcm4pv/aO2K+HHAb9yzhUB+4FLIvrTiHRBVxaLdGJmNc45T4DtW4AvOOc2+RcU3OmcG2pmFcBI51yTf/sO51yOmZUDBc65hg7HKARe89/cBTO7A0h2zv2oD340kYDUIxDpHhfkebA2gTR0eN6CxuokyhQEIt0zu8PXv/ufv8vnt5u8AnjH/3wx8DVovw90Vl8VKdId+iQicqhBZrayw/cvO+fappCmmtn7+D5EzfFvuxH4PzO7Dd/d0q72b78JWOBfVbIFXyjsiHTxIt2lMQKRMPnHCEqccxXRrkWkN+nUkIhInFOPQEQkzqlHICIS5xQEIiJxTkEgIhLnFAQiInFOQSAiEuf+P1RXQm27K/FYAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from layers import DropoutLayer\n",
    "dropout_rate = 0.5\n",
    "batch_size = 100\n",
    "max_epoch = 15\n",
    "init_std = 0.01\n",
    "\n",
    "learning_rate = 0.0025\n",
    "weight_decay = 0.010\n",
    "dropout_rate = 0.25\n",
    "\n",
    "disp_freq = 10\n",
    "\n",
    "# build your network\n",
    "criterion = SoftmaxCrossEntropyLossLayer()\n",
    "sgd = SGD(learning_rate, weight_decay)\n",
    "\n",
    "dropout = Network()\n",
    "dropout.add(ConvLayer(1, 16, 3, 1))\n",
    "dropout.add(ReLULayer())\n",
    "dropout.add(MaxPoolingLayer(2, 0))\n",
    "dropout.add(ConvLayer(16, 32, 3, 1))\n",
    "dropout.add(ReLULayer())\n",
    "dropout.add(MaxPoolingLayer(2, 0))\n",
    "dropout.add(ReshapeLayer((batch_size, 32, 7, 7), (batch_size, 1568)))\n",
    "dropout.add(FCLayer(1568, 128))\n",
    "dropout.add(ReLULayer())\n",
    "dropout.add(DropoutLayer(dropout_rate))\n",
    "dropout.add(FCLayer(128, 64))\n",
    "dropout.add(ReLULayer())\n",
    "dropout.add(DropoutLayer(dropout_rate))\n",
    "dropout.add(FCLayer(64, 32))\n",
    "dropout.add(ReLULayer())\n",
    "dropout.add(DropoutLayer(dropout_rate))\n",
    "dropout.add(FCLayer(32, 10))\n",
    "\n",
    "\n",
    "\n",
    "# training\n",
    "dropout.is_training = True\n",
    "dropout, train_loss, train_acc, val_loss, val_acc = \\\n",
    "    train(dropout, criterion, sgd, dataset.train, dataset.validation, max_epoch, batch_size, disp_freq)\n",
    "\n",
    "# testing\n",
    "\n",
    "dropout.is_training = False\n",
    "test(dropout, criterion, dataset.test, batch_size, disp_freq)\n",
    "\n",
    "#Plot\n",
    "\n",
    "plot_loss_and_acc({\n",
    "    'Training': [train_loss, train_acc], \n",
    "    'Validation': [val_loss, val_acc]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
